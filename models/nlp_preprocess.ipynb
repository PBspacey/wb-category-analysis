{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaEJuGU90sFx",
        "outputId": "88c0467b-6dc7-40f1-c2cc-b71572b9733b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy3) (0.7.2)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.10/dist-packages (from pymorphy3) (2.4.417150.4580142)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy3\n",
        "!pip install catboost\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "QXL15H_P0PZp"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from catboost import CatBoostClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b26ZUP580PZr"
      },
      "source": [
        "## Preprocess texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz8wBx7h0kyp",
        "outputId": "4a86c93c-0dec-4538-872f-ecdb75ab8c11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "gicC2njh1UR1"
      },
      "outputs": [],
      "source": [
        "# path = '/content/drive/My Drive/Term/texts_labeled.tsv'\n",
        "path = '/Users/nikitasenyatkin/Documents/Мага/2 курс/Thesis/wb-category-analysis/texts_labeled.tsv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "PqjiTrzX0PZr"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(path, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "pc7h03u-0PZs",
        "outputId": "6dae3c8d-a202-4647-b6c1-09145ff41c8a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>INPUT:comment</th>\n",
              "      <th>OUTPUT:is_infringements</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\r\\nВРЕМЯ ПИТЬ ЧАЙ!\\r\\nBrusnikaTea «ЯГОДНЫЙ» -...</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\r\\nВкусный чай зеленый листовой ягодами земля...</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\r\\nИван чай Глазова Гора – традиционный русск...</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\r\\nИщете идеальный подарок на Новый год? Пода...</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\r\\nНастоящий Молочный улун высший сорт из Кит...</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003</th>\n",
              "      <td>Яркий фруктово-ягодный купаж на основе облепих...</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004</th>\n",
              "      <td>Ярко выраженное восточное благоухание зелёного...</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005</th>\n",
              "      <td>пакетиках 14 матча день зеленый черный рождени...</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006</th>\n",
              "      <td>улун дыня  - разновидность крупнолистового чая...</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007</th>\n",
              "      <td>чай чёрный, рассыпной, масала, специи, саган д...</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2008 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          INPUT:comment  \\\n",
              "0     \\r\\nВРЕМЯ ПИТЬ ЧАЙ!\\r\\nBrusnikaTea «ЯГОДНЫЙ» -...   \n",
              "1     \\r\\nВкусный чай зеленый листовой ягодами земля...   \n",
              "2     \\r\\nИван чай Глазова Гора – традиционный русск...   \n",
              "3     \\r\\nИщете идеальный подарок на Новый год? Пода...   \n",
              "4     \\r\\nНастоящий Молочный улун высший сорт из Кит...   \n",
              "...                                                 ...   \n",
              "2003  Яркий фруктово-ягодный купаж на основе облепих...   \n",
              "2004  Ярко выраженное восточное благоухание зелёного...   \n",
              "2005  пакетиках 14 матча день зеленый черный рождени...   \n",
              "2006  улун дыня  - разновидность крупнолистового чая...   \n",
              "2007  чай чёрный, рассыпной, масала, специи, саган д...   \n",
              "\n",
              "     OUTPUT:is_infringements  \n",
              "0                        yes  \n",
              "1                         no  \n",
              "2                         no  \n",
              "3                         no  \n",
              "4                        yes  \n",
              "...                      ...  \n",
              "2003                     yes  \n",
              "2004                      no  \n",
              "2005                      no  \n",
              "2006                      no  \n",
              "2007                      no  \n",
              "\n",
              "[2008 rows x 2 columns]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res = df.groupby('INPUT:comment')['OUTPUT:is_infringements'].apply(lambda row: row.value_counts().idxmax()).reset_index()\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "KyLygz5A0PZs",
        "outputId": "e92fee8d-deeb-485a-c056-47eb55b319e6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>INPUT:comment</th>\n",
              "      <th>OUTPUT:insult</th>\n",
              "      <th>OUTPUT:nonsense</th>\n",
              "      <th>OUTPUT:profanity</th>\n",
              "      <th>OUTPUT:advertising</th>\n",
              "      <th>OUTPUT:law_violation</th>\n",
              "      <th>OUTPUT:is_infringements</th>\n",
              "      <th>GOLDEN:insult</th>\n",
              "      <th>GOLDEN:nonsense</th>\n",
              "      <th>GOLDEN:profanity</th>\n",
              "      <th>...</th>\n",
              "      <th>GOLDEN:law_violation</th>\n",
              "      <th>GOLDEN:is_infringements</th>\n",
              "      <th>HINT:text</th>\n",
              "      <th>HINT:default_language</th>\n",
              "      <th>ASSIGNMENT:link</th>\n",
              "      <th>ASSIGNMENT:task_id</th>\n",
              "      <th>ASSIGNMENT:assignment_id</th>\n",
              "      <th>ASSIGNMENT:worker_id</th>\n",
              "      <th>ASSIGNMENT:status</th>\n",
              "      <th>ASSIGNMENT:started</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4853</th>\n",
              "      <td>\\r\\nВкусный чай зеленый листовой ягодами земля...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://platform.toloka.ai/task/42935533/00028...</td>\n",
              "      <td>00028f24ed--661febdf1c7df82555cadd30</td>\n",
              "      <td>00028f24ed--661ff1997d5f9b2330ba2f9c</td>\n",
              "      <td>5f271f4fb1d5d003e454617253cc57d8</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2024-04-17T15:58:17.613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4901</th>\n",
              "      <td>\\r\\nВкусный чай зеленый листовой ягодами земля...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://platform.toloka.ai/task/42935533/00028...</td>\n",
              "      <td>00028f24ed--661febdf1c7df82555cadd30</td>\n",
              "      <td>00028f24ed--661ff19bdaf59030df3c69c2</td>\n",
              "      <td>bfdb99f582393c82a7545545bc7044fb</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2024-04-17T15:58:19.082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4953</th>\n",
              "      <td>\\r\\nВкусный чай зеленый листовой ягодами земля...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://platform.toloka.ai/task/42935533/00028...</td>\n",
              "      <td>00028f24ed--661febdf1c7df82555cadd30</td>\n",
              "      <td>00028f24ed--661ff19cdaf59030df3c69f7</td>\n",
              "      <td>2a38e4d4e4a4736d579c9e52f33b9075</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2024-04-17T15:58:20.522</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          INPUT:comment OUTPUT:insult  \\\n",
              "4853  \\r\\nВкусный чай зеленый листовой ягодами земля...           NaN   \n",
              "4901  \\r\\nВкусный чай зеленый листовой ягодами земля...           NaN   \n",
              "4953  \\r\\nВкусный чай зеленый листовой ягодами земля...           NaN   \n",
              "\n",
              "     OUTPUT:nonsense OUTPUT:profanity OUTPUT:advertising OUTPUT:law_violation  \\\n",
              "4853             NaN              NaN                NaN                  NaN   \n",
              "4901             NaN              NaN               True                  NaN   \n",
              "4953             NaN              NaN                NaN                  NaN   \n",
              "\n",
              "     OUTPUT:is_infringements  GOLDEN:insult  GOLDEN:nonsense  \\\n",
              "4853                      no            NaN              NaN   \n",
              "4901                     yes            NaN              NaN   \n",
              "4953                      no            NaN              NaN   \n",
              "\n",
              "      GOLDEN:profanity  ...  GOLDEN:law_violation  GOLDEN:is_infringements  \\\n",
              "4853               NaN  ...                   NaN                      NaN   \n",
              "4901               NaN  ...                   NaN                      NaN   \n",
              "4953               NaN  ...                   NaN                      NaN   \n",
              "\n",
              "      HINT:text  HINT:default_language  \\\n",
              "4853        NaN                    NaN   \n",
              "4901        NaN                    NaN   \n",
              "4953        NaN                    NaN   \n",
              "\n",
              "                                        ASSIGNMENT:link  \\\n",
              "4853  https://platform.toloka.ai/task/42935533/00028...   \n",
              "4901  https://platform.toloka.ai/task/42935533/00028...   \n",
              "4953  https://platform.toloka.ai/task/42935533/00028...   \n",
              "\n",
              "                        ASSIGNMENT:task_id  \\\n",
              "4853  00028f24ed--661febdf1c7df82555cadd30   \n",
              "4901  00028f24ed--661febdf1c7df82555cadd30   \n",
              "4953  00028f24ed--661febdf1c7df82555cadd30   \n",
              "\n",
              "                  ASSIGNMENT:assignment_id              ASSIGNMENT:worker_id  \\\n",
              "4853  00028f24ed--661ff1997d5f9b2330ba2f9c  5f271f4fb1d5d003e454617253cc57d8   \n",
              "4901  00028f24ed--661ff19bdaf59030df3c69c2  bfdb99f582393c82a7545545bc7044fb   \n",
              "4953  00028f24ed--661ff19cdaf59030df3c69f7  2a38e4d4e4a4736d579c9e52f33b9075   \n",
              "\n",
              "     ASSIGNMENT:status       ASSIGNMENT:started  \n",
              "4853          APPROVED  2024-04-17T15:58:17.613  \n",
              "4901          APPROVED  2024-04-17T15:58:19.082  \n",
              "4953          APPROVED  2024-04-17T15:58:20.522  \n",
              "\n",
              "[3 rows x 21 columns]"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[df['INPUT:comment'] == '\\r\\nВкусный чай зеленый листовой ягодами земляники. Земляничный улун подарочный. Улун китайский и ягоды сушеные (земляника сублимированная) возвратят Вас в лето и превратят чаепитие в праздник. Сублимированные ягоды и крупнолистовой зелёный чай укрепят здоровье. Содержит витамины D, C, E, K, марганец, фосфор, железо и йод. Чай заварной с натуральными добавками выводит шлаки и токсины из организма. Напиток можно использовать для детокса и похудения. Чай с ягодами имеет обволакивающий вкус и нежный аромат. Фруктовый чай натуральный антиоксидант, полезен для похудения, укрепляет костные ткани. Чайное ассорти изготовлено без искусственных красителей и не содержит ГМО. Чай подарочный ягодный микс в подарок женщине, мужчине. Чай с фруктами можно подать во время застолья на новогодний праздник. Оригинальный и нежный вкус наверняка понравится гостям. Состав: чай китайский улун, ягоды земляники, лист брусники, натуральный земляничный ароматизатор. После него Вы забудете про чай в пакетиках. сушеные ягоды, лепестки и травы - чайный напиток с натуральными витаминами и полезными минералами. Практичный, универсальный подарок с пользой для здоровья удивит и вызовет положительные эмоции, а также даст повод собраться с друзьями и родными за чашечкой чая. После него Вы забудете про чай в пакетиках. Тонизирующее, так и успокаивающее воздействие. ']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "teSKIxro0PZt"
      },
      "outputs": [],
      "source": [
        "names = {\n",
        "    'INPUT:comment' : 'comment',\n",
        "    'OUTPUT:is_infringements' : 'target'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "BSAsERAc0PZt"
      },
      "outputs": [],
      "source": [
        "res.rename(names, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mht9D3tV0PZt"
      },
      "outputs": [],
      "source": [
        "# df['описание'] = df['описание'].fillna('Описания нет')\n",
        "# df['бренд'] = df['бренд'].fillna('Бренда нет')\n",
        "\n",
        "# df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjPPOMFk2ukU",
        "outputId": "be0f05f6-ea7c-4886-d319-d1b5db117819"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/nikitasenyatkin/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "aLDIrENd0PZt"
      },
      "outputs": [],
      "source": [
        "patterns = '[^0-9а-яА-ЯёЁ\\s]+'\n",
        "stopwords_ru = stopwords.words(\"russian\")\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "def lemmatize(doc):\n",
        "    doc = re.sub(patterns, ' ', doc)\n",
        "    tokens = []\n",
        "    for token in doc.split():\n",
        "        if token and token not in stopwords_ru:\n",
        "            token = token.strip()\n",
        "            token = morph.normal_forms(token)[0]\n",
        "\n",
        "            tokens.append(token)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Vv7badRI0PZu"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(row):\n",
        "\n",
        "    text = str(row).lower()\n",
        "\n",
        "    # deleting punctuation\n",
        "    punc = str.maketrans('', '', string.punctuation)\n",
        "    text_no_punct = text.translate(punc)\n",
        "\n",
        "    # lemmatizing\n",
        "    lematized_text = lemmatize(text_no_punct)\n",
        "\n",
        "    return lematized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "R7V_3St10PZu"
      },
      "outputs": [],
      "source": [
        "def target_labeler(s):\n",
        "    if s == 'yes':\n",
        "        return 1\n",
        "    if s == 'avg':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "03Yrbqb70PZu"
      },
      "outputs": [],
      "source": [
        "preprocessed_df = res.copy()\n",
        "preprocessed_df['comment'] = res['comment'].apply(preprocess_text)\n",
        "preprocessed_df['target'] = preprocessed_df['target'].apply(lambda row: target_labeler(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr1e1K_j0PZw"
      },
      "source": [
        "Опробовать сетки отсюда, если нет, то гг, придется что- то думать.\n",
        "В противном случае генерация описаний или придумывание описательных фичей текста, которые могут помочь, вместо стандартных векторов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXwiPUwr0PZw"
      },
      "source": [
        "## Training embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "_m1eZYzS0PZw"
      },
      "outputs": [],
      "source": [
        "# creating tagged data\n",
        "tagged_data = [TaggedDocument(words=d, tags=[str(i)]) for i, d in enumerate(preprocessed_df['comment'])]\n",
        "\n",
        "# creating model\n",
        "model = Doc2Vec(vector_size=25, window=2, min_count=1, workers=4, seed=42)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=5)\n",
        "\n",
        "# receiving vecs\n",
        "vectors = pd.DataFrame(model.infer_vector(doc.words) for doc in tagged_data)\n",
        "# vectors = pd.DataFrame(model.dv.get_vector(str(i)) for i in range(len(tagged_data)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "cWv12I700PZw"
      },
      "outputs": [],
      "source": [
        "def target_labeler(s):\n",
        "    if s == 'yes':\n",
        "        return 1\n",
        "    if s == 'avg':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def counter(class_name):\n",
        "    return preprocessed_df['target'].count()/preprocessed_df['target'][preprocessed_df['target']==class_name].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "6LLX6mtE0PZw",
        "outputId": "39d50d86-e574-40dc-f7b3-f38b8ceb89c7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.220616</td>\n",
              "      <td>-0.584180</td>\n",
              "      <td>0.124026</td>\n",
              "      <td>-0.678434</td>\n",
              "      <td>-0.596027</td>\n",
              "      <td>-0.351055</td>\n",
              "      <td>0.198904</td>\n",
              "      <td>0.148530</td>\n",
              "      <td>-0.036486</td>\n",
              "      <td>-0.343035</td>\n",
              "      <td>...</td>\n",
              "      <td>0.378401</td>\n",
              "      <td>-0.093616</td>\n",
              "      <td>-0.165475</td>\n",
              "      <td>0.021340</td>\n",
              "      <td>0.074733</td>\n",
              "      <td>0.067088</td>\n",
              "      <td>0.943430</td>\n",
              "      <td>0.399883</td>\n",
              "      <td>0.436314</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.128827</td>\n",
              "      <td>-0.265107</td>\n",
              "      <td>-0.117160</td>\n",
              "      <td>-0.090392</td>\n",
              "      <td>-0.159758</td>\n",
              "      <td>0.032678</td>\n",
              "      <td>-0.120282</td>\n",
              "      <td>0.066013</td>\n",
              "      <td>0.113221</td>\n",
              "      <td>-0.211117</td>\n",
              "      <td>...</td>\n",
              "      <td>0.281724</td>\n",
              "      <td>-0.111365</td>\n",
              "      <td>-0.254634</td>\n",
              "      <td>0.176622</td>\n",
              "      <td>0.221308</td>\n",
              "      <td>0.227323</td>\n",
              "      <td>0.321986</td>\n",
              "      <td>-0.027528</td>\n",
              "      <td>0.218121</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.046367</td>\n",
              "      <td>-0.236453</td>\n",
              "      <td>0.107440</td>\n",
              "      <td>-0.250737</td>\n",
              "      <td>-0.329269</td>\n",
              "      <td>-0.115869</td>\n",
              "      <td>-0.000768</td>\n",
              "      <td>0.403920</td>\n",
              "      <td>0.072042</td>\n",
              "      <td>-0.002583</td>\n",
              "      <td>...</td>\n",
              "      <td>0.469560</td>\n",
              "      <td>-0.138503</td>\n",
              "      <td>-0.374971</td>\n",
              "      <td>-0.179433</td>\n",
              "      <td>-0.021338</td>\n",
              "      <td>0.390777</td>\n",
              "      <td>0.486149</td>\n",
              "      <td>0.366084</td>\n",
              "      <td>0.147256</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.257964</td>\n",
              "      <td>-0.313339</td>\n",
              "      <td>-0.624738</td>\n",
              "      <td>-0.456974</td>\n",
              "      <td>-0.460429</td>\n",
              "      <td>-0.081196</td>\n",
              "      <td>-0.246090</td>\n",
              "      <td>0.106785</td>\n",
              "      <td>0.059004</td>\n",
              "      <td>-0.546147</td>\n",
              "      <td>...</td>\n",
              "      <td>0.360625</td>\n",
              "      <td>-0.486059</td>\n",
              "      <td>-0.406992</td>\n",
              "      <td>0.390301</td>\n",
              "      <td>0.248420</td>\n",
              "      <td>-0.103654</td>\n",
              "      <td>0.711125</td>\n",
              "      <td>0.094677</td>\n",
              "      <td>0.738721</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.064954</td>\n",
              "      <td>-0.410846</td>\n",
              "      <td>0.429969</td>\n",
              "      <td>-0.065224</td>\n",
              "      <td>0.154312</td>\n",
              "      <td>-0.283735</td>\n",
              "      <td>-0.110351</td>\n",
              "      <td>0.021634</td>\n",
              "      <td>-0.180701</td>\n",
              "      <td>-0.351758</td>\n",
              "      <td>...</td>\n",
              "      <td>0.178662</td>\n",
              "      <td>-0.158495</td>\n",
              "      <td>-0.281292</td>\n",
              "      <td>0.095516</td>\n",
              "      <td>-0.242509</td>\n",
              "      <td>0.141835</td>\n",
              "      <td>0.559364</td>\n",
              "      <td>0.448124</td>\n",
              "      <td>-0.043176</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003</th>\n",
              "      <td>-0.014613</td>\n",
              "      <td>-0.363927</td>\n",
              "      <td>0.271316</td>\n",
              "      <td>-0.370102</td>\n",
              "      <td>-0.409500</td>\n",
              "      <td>-0.242177</td>\n",
              "      <td>-0.092273</td>\n",
              "      <td>0.215162</td>\n",
              "      <td>-0.204326</td>\n",
              "      <td>-0.190421</td>\n",
              "      <td>...</td>\n",
              "      <td>0.123317</td>\n",
              "      <td>0.112234</td>\n",
              "      <td>-0.161625</td>\n",
              "      <td>-0.092155</td>\n",
              "      <td>0.134661</td>\n",
              "      <td>0.354047</td>\n",
              "      <td>0.520094</td>\n",
              "      <td>0.346471</td>\n",
              "      <td>0.117331</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004</th>\n",
              "      <td>0.046938</td>\n",
              "      <td>-0.297665</td>\n",
              "      <td>0.232149</td>\n",
              "      <td>-0.197294</td>\n",
              "      <td>-0.191693</td>\n",
              "      <td>-0.173330</td>\n",
              "      <td>0.008265</td>\n",
              "      <td>0.032875</td>\n",
              "      <td>-0.046922</td>\n",
              "      <td>-0.092485</td>\n",
              "      <td>...</td>\n",
              "      <td>0.138677</td>\n",
              "      <td>-0.035957</td>\n",
              "      <td>-0.073509</td>\n",
              "      <td>-0.081550</td>\n",
              "      <td>0.077154</td>\n",
              "      <td>0.179880</td>\n",
              "      <td>0.384958</td>\n",
              "      <td>0.243841</td>\n",
              "      <td>0.060791</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005</th>\n",
              "      <td>-0.665997</td>\n",
              "      <td>0.117459</td>\n",
              "      <td>-0.734903</td>\n",
              "      <td>0.022810</td>\n",
              "      <td>0.325615</td>\n",
              "      <td>-0.260974</td>\n",
              "      <td>-0.463145</td>\n",
              "      <td>0.037628</td>\n",
              "      <td>0.042585</td>\n",
              "      <td>-0.019466</td>\n",
              "      <td>...</td>\n",
              "      <td>0.063010</td>\n",
              "      <td>-0.297724</td>\n",
              "      <td>-0.361819</td>\n",
              "      <td>-0.093339</td>\n",
              "      <td>0.045940</td>\n",
              "      <td>-0.230155</td>\n",
              "      <td>-0.243589</td>\n",
              "      <td>0.121035</td>\n",
              "      <td>0.589010</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006</th>\n",
              "      <td>-0.014592</td>\n",
              "      <td>-0.348805</td>\n",
              "      <td>0.291584</td>\n",
              "      <td>-0.126389</td>\n",
              "      <td>-0.003936</td>\n",
              "      <td>-0.462899</td>\n",
              "      <td>-0.046087</td>\n",
              "      <td>0.010926</td>\n",
              "      <td>-0.112173</td>\n",
              "      <td>-0.075841</td>\n",
              "      <td>...</td>\n",
              "      <td>0.243690</td>\n",
              "      <td>-0.022741</td>\n",
              "      <td>-0.411017</td>\n",
              "      <td>-0.100271</td>\n",
              "      <td>-0.295122</td>\n",
              "      <td>0.075646</td>\n",
              "      <td>0.530281</td>\n",
              "      <td>0.499044</td>\n",
              "      <td>0.062278</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007</th>\n",
              "      <td>-0.061847</td>\n",
              "      <td>-0.079912</td>\n",
              "      <td>0.015508</td>\n",
              "      <td>-0.091120</td>\n",
              "      <td>-0.043321</td>\n",
              "      <td>-0.074290</td>\n",
              "      <td>-0.065697</td>\n",
              "      <td>0.119223</td>\n",
              "      <td>-0.092779</td>\n",
              "      <td>0.009863</td>\n",
              "      <td>...</td>\n",
              "      <td>0.042437</td>\n",
              "      <td>0.023063</td>\n",
              "      <td>-0.081816</td>\n",
              "      <td>-0.099599</td>\n",
              "      <td>0.040012</td>\n",
              "      <td>0.126734</td>\n",
              "      <td>0.107453</td>\n",
              "      <td>0.059813</td>\n",
              "      <td>0.114856</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2008 rows × 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "0     0.220616 -0.584180  0.124026 -0.678434 -0.596027 -0.351055  0.198904   \n",
              "1    -0.128827 -0.265107 -0.117160 -0.090392 -0.159758  0.032678 -0.120282   \n",
              "2    -0.046367 -0.236453  0.107440 -0.250737 -0.329269 -0.115869 -0.000768   \n",
              "3    -0.257964 -0.313339 -0.624738 -0.456974 -0.460429 -0.081196 -0.246090   \n",
              "4     0.064954 -0.410846  0.429969 -0.065224  0.154312 -0.283735 -0.110351   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "2003 -0.014613 -0.363927  0.271316 -0.370102 -0.409500 -0.242177 -0.092273   \n",
              "2004  0.046938 -0.297665  0.232149 -0.197294 -0.191693 -0.173330  0.008265   \n",
              "2005 -0.665997  0.117459 -0.734903  0.022810  0.325615 -0.260974 -0.463145   \n",
              "2006 -0.014592 -0.348805  0.291584 -0.126389 -0.003936 -0.462899 -0.046087   \n",
              "2007 -0.061847 -0.079912  0.015508 -0.091120 -0.043321 -0.074290 -0.065697   \n",
              "\n",
              "             7         8         9  ...        16        17        18  \\\n",
              "0     0.148530 -0.036486 -0.343035  ...  0.378401 -0.093616 -0.165475   \n",
              "1     0.066013  0.113221 -0.211117  ...  0.281724 -0.111365 -0.254634   \n",
              "2     0.403920  0.072042 -0.002583  ...  0.469560 -0.138503 -0.374971   \n",
              "3     0.106785  0.059004 -0.546147  ...  0.360625 -0.486059 -0.406992   \n",
              "4     0.021634 -0.180701 -0.351758  ...  0.178662 -0.158495 -0.281292   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "2003  0.215162 -0.204326 -0.190421  ...  0.123317  0.112234 -0.161625   \n",
              "2004  0.032875 -0.046922 -0.092485  ...  0.138677 -0.035957 -0.073509   \n",
              "2005  0.037628  0.042585 -0.019466  ...  0.063010 -0.297724 -0.361819   \n",
              "2006  0.010926 -0.112173 -0.075841  ...  0.243690 -0.022741 -0.411017   \n",
              "2007  0.119223 -0.092779  0.009863  ...  0.042437  0.023063 -0.081816   \n",
              "\n",
              "            19        20        21        22        23        24  target  \n",
              "0     0.021340  0.074733  0.067088  0.943430  0.399883  0.436314       1  \n",
              "1     0.176622  0.221308  0.227323  0.321986 -0.027528  0.218121       0  \n",
              "2    -0.179433 -0.021338  0.390777  0.486149  0.366084  0.147256       0  \n",
              "3     0.390301  0.248420 -0.103654  0.711125  0.094677  0.738721       0  \n",
              "4     0.095516 -0.242509  0.141835  0.559364  0.448124 -0.043176       1  \n",
              "...        ...       ...       ...       ...       ...       ...     ...  \n",
              "2003 -0.092155  0.134661  0.354047  0.520094  0.346471  0.117331       1  \n",
              "2004 -0.081550  0.077154  0.179880  0.384958  0.243841  0.060791       0  \n",
              "2005 -0.093339  0.045940 -0.230155 -0.243589  0.121035  0.589010       0  \n",
              "2006 -0.100271 -0.295122  0.075646  0.530281  0.499044  0.062278       0  \n",
              "2007 -0.099599  0.040012  0.126734  0.107453  0.059813  0.114856       0  \n",
              "\n",
              "[2008 rows x 26 columns]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cb_df = pd.concat([vectors, preprocessed_df['target']], axis=1)\n",
        "# cb_df['target'] = cb_df['target'].apply(lambda row: target_labeler(row))\n",
        "# weights = [counter(0), counter(1), counter(2)]\n",
        "cb_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRQcAEE20PZw",
        "outputId": "14434cee-1034-4335-ef84-cfde3e660fb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    1307\n",
              "1     701\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cb_df.target.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "0DWS5RMS6Mrp"
      },
      "outputs": [],
      "source": [
        "X = cb_df.drop('target', axis=1)\n",
        "\n",
        "positive_indices = [i for i, label in enumerate(cb_df['target']) if label == 1]\n",
        "negative_indices = [i for i, label in enumerate(cb_df['target']) if label == 0]\n",
        "\n",
        "positive_vectors = X.loc[positive_indices]\n",
        "negative_vectors = X.loc[negative_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqySuYjQBm8-"
      },
      "source": [
        "KNN for filtering data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "ziEyAd9p7H98"
      },
      "outputs": [],
      "source": [
        "def knn_pos_filtering(positive_vectors, negative_vectors, thresh):\n",
        "\n",
        "  knn_model_pos = NearestNeighbors(n_neighbors=3, algorithm='ball_tree')\n",
        "  knn_model_pos.fit(positive_vectors)\n",
        "\n",
        "  distances, indices = knn_model_pos.kneighbors(negative_vectors)\n",
        "\n",
        "  result = set()\n",
        "  for i, (s, dist, idx) in enumerate(zip(negative_indices, distances, indices)):\n",
        "      if dist.mean() < thresh:\n",
        "        result.add(s)\n",
        "  return list(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBPVu0B1Epgm",
        "outputId": "28cf1eea-a0d5-4afa-d050-e93a1d70e365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "647\n",
            "target\n",
            "0    647\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "pos_filtering_bad_idx = knn_pos_filtering(positive_vectors, negative_vectors, 0.52)\n",
        "print(len(pos_filtering_bad_idx))\n",
        "print(preprocessed_df.loc[pos_filtering_bad_idx]['target'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "HENP54A_FCDJ"
      },
      "outputs": [],
      "source": [
        "def knn_neg_filtering(positive_vectors, negative_vectors, thresh):\n",
        "\n",
        "  knn_model_pos = NearestNeighbors(n_neighbors=3, algorithm='ball_tree')\n",
        "  knn_model_pos.fit(negative_vectors)\n",
        "\n",
        "  distances, indices = knn_model_pos.kneighbors(positive_vectors)\n",
        "\n",
        "  result = set()\n",
        "  for distance in range(len(distances)):\n",
        "    for i in range(3):\n",
        "      if distances[distance][i] < thresh:\n",
        "        result.add(negative_indices[indices[distance][i]])\n",
        "  return list(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SORwrTgFczs",
        "outputId": "12e0161b-c4e5-4410-f5c4-db60e7708c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "865\n",
            "target\n",
            "0    865\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "neg_filtering_bad_idx = knn_neg_filtering(positive_vectors, negative_vectors, 1.3)\n",
        "print(len(neg_filtering_bad_idx))\n",
        "print(preprocessed_df.loc[neg_filtering_bad_idx]['target'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPHzyBbfD7pQ",
        "outputId": "745473f0-25e0-49d6-cbb8-4ebe35c4104d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "1    701\n",
              "0    660\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# res.drop(neg_filtering_bad_idx)['target'].value_counts()\n",
        "cb_df.drop(pos_filtering_bad_idx)['target'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOouMZtIFxhA"
      },
      "source": [
        "## Logistic regression on doc2vec test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "aAL3JtpfCWrX"
      },
      "outputs": [],
      "source": [
        "df = cb_df.drop(pos_filtering_bad_idx)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Lj2HaV9C1w_",
        "outputId": "27d723c3-2186-4b40-ce0b-e85260fd070e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.652014652014652\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_4hcjgZS4Av"
      },
      "source": [
        "## Catboost + Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pW2EkmmSh1S",
        "outputId": "7c623841-a755-4013-8ec6-c66e3c51bf1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.663003663003663\n"
          ]
        }
      ],
      "source": [
        "cb = CatBoostClassifier(early_stopping_rounds=100, learning_rate=0.01)\n",
        "cb.fit(X_train, y_train, verbose=0)\n",
        "pred = cb.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "vBQKHvGMTg2M",
        "outputId": "342398b9-4dd1-49af-a1a0-957a22fa9d54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7355088020609705\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x174ce6c90>]"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pred = cb.predict_proba(X_test)\n",
        "\n",
        "print(roc_auc_score(y_test, pred[:,1]))\n",
        "\n",
        "fpr, tpr, thres = roc_curve(y_test, pred[:,1])\n",
        "\n",
        "plt.plot(fpr, tpr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBePMdEaF_n8"
      },
      "source": [
        "Logistic regression + doc2vec filtration + TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "RNXp5e9gGEmG"
      },
      "outputs": [],
      "source": [
        "tf_data = preprocessed_df.drop(pos_filtering_bad_idx)\n",
        "tf_data['comment'] = tf_data['comment'].apply(lambda row: ' '.join(row))\n",
        "X = tf_data.drop('target', axis=1)\n",
        "y = tf_data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "BSjOrTcFGjQ4"
      },
      "outputs": [],
      "source": [
        "tfidf_1 = TfidfVectorizer(ngram_range=(1, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kch-MtL5GqC9",
        "outputId": "cb917589-9952-47c8-dc7b-30896346ab6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1088, 9124)\n",
            "(273, 9124)\n"
          ]
        }
      ],
      "source": [
        "tfidf_train = tfidf_1.fit_transform(X_train['comment'])\n",
        "tfidf_test = tfidf_1.transform(X_test['comment'])\n",
        "\n",
        "print(tfidf_train.shape)\n",
        "print(tfidf_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92Uw27zZGv3d",
        "outputId": "bcbe260f-2eb3-4374-fa95-69cf5b9760f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6593406593406593\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(tfidf_train, y_train)\n",
        "pred = model.predict(tfidf_test)\n",
        "\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM-W7YOyUwvP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt7eusZUUmn-"
      },
      "source": [
        "## CATBOOST + TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txe9rsIkVGW7",
        "outputId": "6d4dcc84-50d3-4e88-fcf0-258aeee60966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6897810218978102\n"
          ]
        }
      ],
      "source": [
        "cb = CatBoostClassifier(early_stopping_rounds=100, learning_rate=0.01)\n",
        "cb.fit(tfidf_train, y_train, verbose=0)\n",
        "pred = cb.predict(tfidf_test)\n",
        "\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "J5TMXyXSWju_",
        "outputId": "347c608e-d4b3-4a8d-e28d-6f2353335e3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7583266719957368\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x79874995b2b0>]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pred = cb.predict_proba(tfidf_test)\n",
        "\n",
        "print(roc_auc_score(y_test, pred[:,1]))\n",
        "\n",
        "fpr, tpr, thres = roc_curve(y_test, pred[:,1])\n",
        "\n",
        "plt.plot(fpr, tpr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHKcOxzcUo5a"
      },
      "source": [
        "TFIDF works worse, so let's try DOC2VEC + LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O--zLA2dWvGO"
      },
      "source": [
        "## FC + DOC2VEC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "cj-DuaAnXCZq"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "KdnObk5UYHjP"
      },
      "outputs": [],
      "source": [
        "l_df = cb_df.copy()\n",
        "X = l_df.drop('target', axis=1)\n",
        "y = l_df['target']\n",
        "\n",
        "features = X.values\n",
        "targets = y.values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, targets, random_state=42, test_size=0.8)\n",
        "\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "aeCM1zZQWyUl"
      },
      "outputs": [],
      "source": [
        "vocab_size = 2000\n",
        "input_len = 25\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 2\n",
        "n_epochs = 5\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "WSz66WyvunPQ"
      },
      "outputs": [],
      "source": [
        "class FC(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(FC, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128, bias=True)\n",
        "        self.fc2 = nn.Linear(128, num_classes, bias=True)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.4)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = F.relu(self.fc1(X))\n",
        "        out = self.dropout1(out)\n",
        "        out = F.relu(self.fc2(out))\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "X0JVsPLXXK8Q"
      },
      "outputs": [],
      "source": [
        "model = FC(input_len, num_classes)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrWvOir7ZpfX",
        "outputId": "654897be-e8c9-40ec-e8fa-5f0d499da60a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN: Epoch 1, Average Loss: 0.6836, Accuracy: 0.6135\n",
            "TEST: Epoch 1, Average Loss: 0.6621, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 2, Average Loss: 0.6680, Accuracy: 0.6209\n",
            "TEST: Epoch 2, Average Loss: 0.6498, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 3, Average Loss: 0.6656, Accuracy: 0.6209\n",
            "TEST: Epoch 3, Average Loss: 0.6479, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 4, Average Loss: 0.6615, Accuracy: 0.6209\n",
            "TEST: Epoch 4, Average Loss: 0.6500, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 5, Average Loss: 0.6609, Accuracy: 0.6209\n",
            "TEST: Epoch 5, Average Loss: 0.6483, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 6, Average Loss: 0.6604, Accuracy: 0.6209\n",
            "TEST: Epoch 6, Average Loss: 0.6453, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 7, Average Loss: 0.6609, Accuracy: 0.6209\n",
            "TEST: Epoch 7, Average Loss: 0.6437, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 8, Average Loss: 0.6579, Accuracy: 0.6209\n",
            "TEST: Epoch 8, Average Loss: 0.6477, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 9, Average Loss: 0.6586, Accuracy: 0.6209\n",
            "TEST: Epoch 9, Average Loss: 0.6460, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 10, Average Loss: 0.6621, Accuracy: 0.6209\n",
            "TEST: Epoch 10, Average Loss: 0.6438, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 11, Average Loss: 0.6581, Accuracy: 0.6209\n",
            "TEST: Epoch 11, Average Loss: 0.6450, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 12, Average Loss: 0.6545, Accuracy: 0.6209\n",
            "TEST: Epoch 12, Average Loss: 0.6451, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 13, Average Loss: 0.6594, Accuracy: 0.6209\n",
            "TEST: Epoch 13, Average Loss: 0.6466, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 14, Average Loss: 0.6551, Accuracy: 0.6209\n",
            "TEST: Epoch 14, Average Loss: 0.6465, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 15, Average Loss: 0.6514, Accuracy: 0.6209\n",
            "TEST: Epoch 15, Average Loss: 0.6469, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 16, Average Loss: 0.6592, Accuracy: 0.6209\n",
            "TEST: Epoch 16, Average Loss: 0.6478, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 17, Average Loss: 0.6544, Accuracy: 0.6209\n",
            "TEST: Epoch 17, Average Loss: 0.6442, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 18, Average Loss: 0.6474, Accuracy: 0.6209\n",
            "TEST: Epoch 18, Average Loss: 0.6454, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 19, Average Loss: 0.6531, Accuracy: 0.6209\n",
            "TEST: Epoch 19, Average Loss: 0.6447, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 20, Average Loss: 0.6486, Accuracy: 0.6209\n",
            "TEST: Epoch 20, Average Loss: 0.6460, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 21, Average Loss: 0.6461, Accuracy: 0.6209\n",
            "TEST: Epoch 21, Average Loss: 0.6449, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 22, Average Loss: 0.6422, Accuracy: 0.6209\n",
            "TEST: Epoch 22, Average Loss: 0.6462, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 23, Average Loss: 0.6485, Accuracy: 0.6209\n",
            "TEST: Epoch 23, Average Loss: 0.6424, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 24, Average Loss: 0.6501, Accuracy: 0.6209\n",
            "TEST: Epoch 24, Average Loss: 0.6462, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 25, Average Loss: 0.6436, Accuracy: 0.6209\n",
            "TEST: Epoch 25, Average Loss: 0.6448, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 26, Average Loss: 0.6514, Accuracy: 0.6209\n",
            "TEST: Epoch 26, Average Loss: 0.6446, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 27, Average Loss: 0.6427, Accuracy: 0.6209\n",
            "TEST: Epoch 27, Average Loss: 0.6446, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 28, Average Loss: 0.6380, Accuracy: 0.6234\n",
            "TEST: Epoch 28, Average Loss: 0.6448, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 29, Average Loss: 0.6407, Accuracy: 0.6234\n",
            "TEST: Epoch 29, Average Loss: 0.6467, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 30, Average Loss: 0.6371, Accuracy: 0.6209\n",
            "TEST: Epoch 30, Average Loss: 0.6464, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 31, Average Loss: 0.6389, Accuracy: 0.6209\n",
            "TEST: Epoch 31, Average Loss: 0.6456, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 32, Average Loss: 0.6393, Accuracy: 0.6209\n",
            "TEST: Epoch 32, Average Loss: 0.6461, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 33, Average Loss: 0.6304, Accuracy: 0.6209\n",
            "TEST: Epoch 33, Average Loss: 0.6449, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 34, Average Loss: 0.6415, Accuracy: 0.6209\n",
            "TEST: Epoch 34, Average Loss: 0.6431, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 35, Average Loss: 0.6332, Accuracy: 0.6234\n",
            "TEST: Epoch 35, Average Loss: 0.6465, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 36, Average Loss: 0.6356, Accuracy: 0.6234\n",
            "TEST: Epoch 36, Average Loss: 0.6470, Accuracy: 0.6590\n",
            "==============================\n",
            "TRAIN: Epoch 37, Average Loss: 0.6308, Accuracy: 0.6259\n",
            "TEST: Epoch 37, Average Loss: 0.6419, Accuracy: 0.6584\n",
            "==============================\n",
            "TRAIN: Epoch 38, Average Loss: 0.6325, Accuracy: 0.6359\n",
            "TEST: Epoch 38, Average Loss: 0.6515, Accuracy: 0.6478\n",
            "==============================\n",
            "TRAIN: Epoch 39, Average Loss: 0.6376, Accuracy: 0.6683\n",
            "TEST: Epoch 39, Average Loss: 0.6497, Accuracy: 0.6490\n",
            "==============================\n",
            "TRAIN: Epoch 40, Average Loss: 0.6240, Accuracy: 0.6534\n",
            "TEST: Epoch 40, Average Loss: 0.6475, Accuracy: 0.6521\n",
            "==============================\n",
            "TRAIN: Epoch 41, Average Loss: 0.6209, Accuracy: 0.6658\n",
            "TEST: Epoch 41, Average Loss: 0.6525, Accuracy: 0.6478\n",
            "==============================\n",
            "TRAIN: Epoch 42, Average Loss: 0.6164, Accuracy: 0.6758\n",
            "TEST: Epoch 42, Average Loss: 0.6461, Accuracy: 0.6509\n",
            "==============================\n",
            "TRAIN: Epoch 43, Average Loss: 0.6159, Accuracy: 0.6559\n",
            "TEST: Epoch 43, Average Loss: 0.6478, Accuracy: 0.6490\n",
            "==============================\n",
            "TRAIN: Epoch 44, Average Loss: 0.6115, Accuracy: 0.6733\n",
            "TEST: Epoch 44, Average Loss: 0.6544, Accuracy: 0.6478\n",
            "==============================\n",
            "TRAIN: Epoch 45, Average Loss: 0.6062, Accuracy: 0.6833\n",
            "TEST: Epoch 45, Average Loss: 0.6552, Accuracy: 0.6428\n",
            "==============================\n",
            "TRAIN: Epoch 46, Average Loss: 0.6121, Accuracy: 0.6683\n",
            "TEST: Epoch 46, Average Loss: 0.6506, Accuracy: 0.6478\n",
            "==============================\n",
            "TRAIN: Epoch 47, Average Loss: 0.5964, Accuracy: 0.6833\n",
            "TEST: Epoch 47, Average Loss: 0.6573, Accuracy: 0.6378\n",
            "==============================\n",
            "TRAIN: Epoch 48, Average Loss: 0.5966, Accuracy: 0.6808\n",
            "TEST: Epoch 48, Average Loss: 0.6564, Accuracy: 0.6441\n",
            "==============================\n",
            "TRAIN: Epoch 49, Average Loss: 0.5991, Accuracy: 0.6658\n",
            "TEST: Epoch 49, Average Loss: 0.6553, Accuracy: 0.6447\n",
            "==============================\n",
            "TRAIN: Epoch 50, Average Loss: 0.5983, Accuracy: 0.6883\n",
            "TEST: Epoch 50, Average Loss: 0.6580, Accuracy: 0.6322\n",
            "==============================\n",
            "TRAIN: Epoch 51, Average Loss: 0.6012, Accuracy: 0.6858\n",
            "TEST: Epoch 51, Average Loss: 0.6567, Accuracy: 0.6441\n",
            "==============================\n",
            "TRAIN: Epoch 52, Average Loss: 0.5910, Accuracy: 0.7107\n",
            "TEST: Epoch 52, Average Loss: 0.6598, Accuracy: 0.6353\n",
            "==============================\n",
            "TRAIN: Epoch 53, Average Loss: 0.5891, Accuracy: 0.7007\n",
            "TEST: Epoch 53, Average Loss: 0.6595, Accuracy: 0.6360\n",
            "==============================\n",
            "TRAIN: Epoch 54, Average Loss: 0.5866, Accuracy: 0.6958\n",
            "TEST: Epoch 54, Average Loss: 0.6628, Accuracy: 0.6279\n",
            "==============================\n",
            "TRAIN: Epoch 55, Average Loss: 0.5842, Accuracy: 0.7032\n",
            "TEST: Epoch 55, Average Loss: 0.6574, Accuracy: 0.6297\n",
            "==============================\n",
            "TRAIN: Epoch 56, Average Loss: 0.5857, Accuracy: 0.7032\n",
            "TEST: Epoch 56, Average Loss: 0.6605, Accuracy: 0.6353\n",
            "==============================\n",
            "TRAIN: Epoch 57, Average Loss: 0.5858, Accuracy: 0.7132\n",
            "TEST: Epoch 57, Average Loss: 0.6630, Accuracy: 0.6204\n",
            "==============================\n",
            "TRAIN: Epoch 58, Average Loss: 0.5772, Accuracy: 0.7107\n",
            "TEST: Epoch 58, Average Loss: 0.6587, Accuracy: 0.6366\n",
            "==============================\n",
            "TRAIN: Epoch 59, Average Loss: 0.5700, Accuracy: 0.7007\n",
            "TEST: Epoch 59, Average Loss: 0.6630, Accuracy: 0.6316\n",
            "==============================\n",
            "TRAIN: Epoch 60, Average Loss: 0.5735, Accuracy: 0.7082\n",
            "TEST: Epoch 60, Average Loss: 0.6645, Accuracy: 0.6304\n",
            "==============================\n",
            "TRAIN: Epoch 61, Average Loss: 0.5734, Accuracy: 0.7007\n",
            "TEST: Epoch 61, Average Loss: 0.6626, Accuracy: 0.6229\n",
            "==============================\n",
            "TRAIN: Epoch 62, Average Loss: 0.5680, Accuracy: 0.7182\n",
            "TEST: Epoch 62, Average Loss: 0.6741, Accuracy: 0.6154\n",
            "==============================\n",
            "TRAIN: Epoch 63, Average Loss: 0.5646, Accuracy: 0.7082\n",
            "TEST: Epoch 63, Average Loss: 0.6684, Accuracy: 0.6347\n",
            "==============================\n",
            "TRAIN: Epoch 64, Average Loss: 0.5681, Accuracy: 0.7132\n",
            "TEST: Epoch 64, Average Loss: 0.6696, Accuracy: 0.6179\n",
            "==============================\n",
            "TRAIN: Epoch 65, Average Loss: 0.5596, Accuracy: 0.7357\n",
            "TEST: Epoch 65, Average Loss: 0.6698, Accuracy: 0.6192\n",
            "==============================\n",
            "TRAIN: Epoch 66, Average Loss: 0.5526, Accuracy: 0.7282\n",
            "TEST: Epoch 66, Average Loss: 0.6708, Accuracy: 0.6291\n",
            "==============================\n",
            "TRAIN: Epoch 67, Average Loss: 0.5544, Accuracy: 0.7157\n",
            "TEST: Epoch 67, Average Loss: 0.6699, Accuracy: 0.6223\n",
            "==============================\n",
            "TRAIN: Epoch 68, Average Loss: 0.5590, Accuracy: 0.7332\n",
            "TEST: Epoch 68, Average Loss: 0.6690, Accuracy: 0.6223\n",
            "==============================\n",
            "TRAIN: Epoch 69, Average Loss: 0.5604, Accuracy: 0.7282\n",
            "TEST: Epoch 69, Average Loss: 0.6676, Accuracy: 0.6235\n",
            "==============================\n",
            "TRAIN: Epoch 70, Average Loss: 0.5399, Accuracy: 0.7456\n",
            "TEST: Epoch 70, Average Loss: 0.6722, Accuracy: 0.6248\n",
            "==============================\n",
            "TRAIN: Epoch 71, Average Loss: 0.5467, Accuracy: 0.7431\n",
            "TEST: Epoch 71, Average Loss: 0.6736, Accuracy: 0.6210\n",
            "==============================\n",
            "TRAIN: Epoch 72, Average Loss: 0.5564, Accuracy: 0.7007\n",
            "TEST: Epoch 72, Average Loss: 0.6735, Accuracy: 0.6329\n",
            "==============================\n",
            "TRAIN: Epoch 73, Average Loss: 0.5419, Accuracy: 0.7182\n",
            "TEST: Epoch 73, Average Loss: 0.6751, Accuracy: 0.6173\n",
            "==============================\n",
            "TRAIN: Epoch 74, Average Loss: 0.5395, Accuracy: 0.7406\n",
            "TEST: Epoch 74, Average Loss: 0.6810, Accuracy: 0.6105\n",
            "==============================\n",
            "TRAIN: Epoch 75, Average Loss: 0.5472, Accuracy: 0.7631\n",
            "TEST: Epoch 75, Average Loss: 0.6791, Accuracy: 0.6123\n",
            "==============================\n",
            "TRAIN: Epoch 76, Average Loss: 0.5311, Accuracy: 0.7531\n",
            "TEST: Epoch 76, Average Loss: 0.6807, Accuracy: 0.6273\n",
            "==============================\n",
            "TRAIN: Epoch 77, Average Loss: 0.5326, Accuracy: 0.7506\n",
            "TEST: Epoch 77, Average Loss: 0.6791, Accuracy: 0.6185\n",
            "==============================\n",
            "TRAIN: Epoch 78, Average Loss: 0.5310, Accuracy: 0.7581\n",
            "TEST: Epoch 78, Average Loss: 0.6827, Accuracy: 0.6185\n",
            "==============================\n",
            "TRAIN: Epoch 79, Average Loss: 0.5259, Accuracy: 0.7731\n",
            "TEST: Epoch 79, Average Loss: 0.6827, Accuracy: 0.6185\n",
            "==============================\n",
            "TRAIN: Epoch 80, Average Loss: 0.5299, Accuracy: 0.7431\n",
            "TEST: Epoch 80, Average Loss: 0.6870, Accuracy: 0.6217\n",
            "==============================\n",
            "TRAIN: Epoch 81, Average Loss: 0.5268, Accuracy: 0.7706\n",
            "TEST: Epoch 81, Average Loss: 0.6859, Accuracy: 0.6105\n",
            "==============================\n",
            "TRAIN: Epoch 82, Average Loss: 0.5140, Accuracy: 0.7631\n",
            "TEST: Epoch 82, Average Loss: 0.6793, Accuracy: 0.6248\n",
            "==============================\n",
            "TRAIN: Epoch 83, Average Loss: 0.5180, Accuracy: 0.7581\n",
            "TEST: Epoch 83, Average Loss: 0.6896, Accuracy: 0.6055\n",
            "==============================\n",
            "TRAIN: Epoch 84, Average Loss: 0.5299, Accuracy: 0.7207\n",
            "TEST: Epoch 84, Average Loss: 0.6858, Accuracy: 0.6229\n",
            "==============================\n",
            "TRAIN: Epoch 85, Average Loss: 0.5173, Accuracy: 0.7631\n",
            "TEST: Epoch 85, Average Loss: 0.6913, Accuracy: 0.6123\n",
            "==============================\n",
            "TRAIN: Epoch 86, Average Loss: 0.5102, Accuracy: 0.7556\n",
            "TEST: Epoch 86, Average Loss: 0.6897, Accuracy: 0.6080\n",
            "==============================\n",
            "TRAIN: Epoch 87, Average Loss: 0.5108, Accuracy: 0.7456\n",
            "TEST: Epoch 87, Average Loss: 0.6941, Accuracy: 0.6185\n",
            "==============================\n",
            "TRAIN: Epoch 88, Average Loss: 0.5253, Accuracy: 0.7481\n",
            "TEST: Epoch 88, Average Loss: 0.6914, Accuracy: 0.6136\n",
            "==============================\n",
            "TRAIN: Epoch 89, Average Loss: 0.4914, Accuracy: 0.7581\n",
            "TEST: Epoch 89, Average Loss: 0.6962, Accuracy: 0.5986\n",
            "==============================\n",
            "TRAIN: Epoch 90, Average Loss: 0.5156, Accuracy: 0.7606\n",
            "TEST: Epoch 90, Average Loss: 0.6999, Accuracy: 0.6024\n",
            "==============================\n",
            "TRAIN: Epoch 91, Average Loss: 0.5176, Accuracy: 0.7606\n",
            "TEST: Epoch 91, Average Loss: 0.6960, Accuracy: 0.6067\n",
            "==============================\n",
            "TRAIN: Epoch 92, Average Loss: 0.5054, Accuracy: 0.7531\n",
            "TEST: Epoch 92, Average Loss: 0.6963, Accuracy: 0.6229\n",
            "==============================\n",
            "TRAIN: Epoch 93, Average Loss: 0.5112, Accuracy: 0.7431\n",
            "TEST: Epoch 93, Average Loss: 0.7042, Accuracy: 0.5937\n",
            "==============================\n",
            "TRAIN: Epoch 94, Average Loss: 0.5144, Accuracy: 0.7531\n",
            "TEST: Epoch 94, Average Loss: 0.6990, Accuracy: 0.5955\n",
            "==============================\n",
            "TRAIN: Epoch 95, Average Loss: 0.5154, Accuracy: 0.7706\n",
            "TEST: Epoch 95, Average Loss: 0.6994, Accuracy: 0.6229\n",
            "==============================\n",
            "TRAIN: Epoch 96, Average Loss: 0.5054, Accuracy: 0.7756\n",
            "TEST: Epoch 96, Average Loss: 0.7013, Accuracy: 0.5993\n",
            "==============================\n",
            "TRAIN: Epoch 97, Average Loss: 0.4956, Accuracy: 0.7731\n",
            "TEST: Epoch 97, Average Loss: 0.7023, Accuracy: 0.5806\n",
            "==============================\n",
            "TRAIN: Epoch 98, Average Loss: 0.4878, Accuracy: 0.7706\n",
            "TEST: Epoch 98, Average Loss: 0.7008, Accuracy: 0.6042\n",
            "==============================\n",
            "TRAIN: Epoch 99, Average Loss: 0.5043, Accuracy: 0.7756\n",
            "TEST: Epoch 99, Average Loss: 0.7036, Accuracy: 0.6179\n",
            "==============================\n",
            "TRAIN: Epoch 100, Average Loss: 0.4848, Accuracy: 0.7706\n",
            "TEST: Epoch 100, Average Loss: 0.7108, Accuracy: 0.5949\n",
            "==============================\n",
            "TRAIN: Epoch 101, Average Loss: 0.4858, Accuracy: 0.7631\n",
            "TEST: Epoch 101, Average Loss: 0.7127, Accuracy: 0.6049\n",
            "==============================\n",
            "TRAIN: Epoch 102, Average Loss: 0.5026, Accuracy: 0.7606\n",
            "TEST: Epoch 102, Average Loss: 0.7087, Accuracy: 0.5986\n",
            "==============================\n",
            "TRAIN: Epoch 103, Average Loss: 0.4949, Accuracy: 0.7830\n",
            "TEST: Epoch 103, Average Loss: 0.7124, Accuracy: 0.5955\n",
            "==============================\n",
            "TRAIN: Epoch 104, Average Loss: 0.4859, Accuracy: 0.7855\n",
            "TEST: Epoch 104, Average Loss: 0.7076, Accuracy: 0.5999\n",
            "==============================\n",
            "TRAIN: Epoch 105, Average Loss: 0.4868, Accuracy: 0.7606\n",
            "TEST: Epoch 105, Average Loss: 0.7142, Accuracy: 0.6260\n",
            "==============================\n",
            "TRAIN: Epoch 106, Average Loss: 0.4852, Accuracy: 0.7531\n",
            "TEST: Epoch 106, Average Loss: 0.7154, Accuracy: 0.5594\n",
            "==============================\n",
            "TRAIN: Epoch 107, Average Loss: 0.4828, Accuracy: 0.7756\n",
            "TEST: Epoch 107, Average Loss: 0.7167, Accuracy: 0.5980\n",
            "==============================\n",
            "TRAIN: Epoch 108, Average Loss: 0.4741, Accuracy: 0.7855\n",
            "TEST: Epoch 108, Average Loss: 0.7165, Accuracy: 0.6111\n",
            "==============================\n",
            "TRAIN: Epoch 109, Average Loss: 0.4859, Accuracy: 0.7880\n",
            "TEST: Epoch 109, Average Loss: 0.7145, Accuracy: 0.5943\n",
            "==============================\n",
            "TRAIN: Epoch 110, Average Loss: 0.4738, Accuracy: 0.7980\n",
            "TEST: Epoch 110, Average Loss: 0.7153, Accuracy: 0.5868\n",
            "==============================\n",
            "TRAIN: Epoch 111, Average Loss: 0.4934, Accuracy: 0.7781\n",
            "TEST: Epoch 111, Average Loss: 0.7235, Accuracy: 0.5825\n",
            "==============================\n",
            "TRAIN: Epoch 112, Average Loss: 0.4635, Accuracy: 0.8105\n",
            "TEST: Epoch 112, Average Loss: 0.7221, Accuracy: 0.5943\n",
            "==============================\n",
            "TRAIN: Epoch 113, Average Loss: 0.4907, Accuracy: 0.7606\n",
            "TEST: Epoch 113, Average Loss: 0.7239, Accuracy: 0.6073\n",
            "==============================\n",
            "TRAIN: Epoch 114, Average Loss: 0.4729, Accuracy: 0.7830\n",
            "TEST: Epoch 114, Average Loss: 0.7230, Accuracy: 0.5700\n",
            "==============================\n",
            "TRAIN: Epoch 115, Average Loss: 0.4853, Accuracy: 0.7781\n",
            "TEST: Epoch 115, Average Loss: 0.7227, Accuracy: 0.6067\n",
            "==============================\n",
            "TRAIN: Epoch 116, Average Loss: 0.4929, Accuracy: 0.7556\n",
            "TEST: Epoch 116, Average Loss: 0.7329, Accuracy: 0.6055\n",
            "==============================\n",
            "TRAIN: Epoch 117, Average Loss: 0.4673, Accuracy: 0.7706\n",
            "TEST: Epoch 117, Average Loss: 0.7236, Accuracy: 0.5918\n",
            "==============================\n",
            "TRAIN: Epoch 118, Average Loss: 0.4690, Accuracy: 0.7830\n",
            "TEST: Epoch 118, Average Loss: 0.7285, Accuracy: 0.5856\n",
            "==============================\n",
            "TRAIN: Epoch 119, Average Loss: 0.4831, Accuracy: 0.7656\n",
            "TEST: Epoch 119, Average Loss: 0.7279, Accuracy: 0.5961\n",
            "==============================\n",
            "TRAIN: Epoch 120, Average Loss: 0.4714, Accuracy: 0.7905\n",
            "TEST: Epoch 120, Average Loss: 0.7299, Accuracy: 0.5719\n",
            "==============================\n",
            "TRAIN: Epoch 121, Average Loss: 0.4737, Accuracy: 0.8005\n",
            "TEST: Epoch 121, Average Loss: 0.7290, Accuracy: 0.5781\n",
            "==============================\n",
            "TRAIN: Epoch 122, Average Loss: 0.4782, Accuracy: 0.7656\n",
            "TEST: Epoch 122, Average Loss: 0.7298, Accuracy: 0.6017\n",
            "==============================\n",
            "TRAIN: Epoch 123, Average Loss: 0.4579, Accuracy: 0.7930\n",
            "TEST: Epoch 123, Average Loss: 0.7253, Accuracy: 0.5899\n",
            "==============================\n",
            "TRAIN: Epoch 124, Average Loss: 0.4630, Accuracy: 0.8005\n",
            "TEST: Epoch 124, Average Loss: 0.7299, Accuracy: 0.5955\n",
            "==============================\n",
            "TRAIN: Epoch 125, Average Loss: 0.4564, Accuracy: 0.7805\n",
            "TEST: Epoch 125, Average Loss: 0.7356, Accuracy: 0.5837\n",
            "==============================\n",
            "TRAIN: Epoch 126, Average Loss: 0.4722, Accuracy: 0.7930\n",
            "TEST: Epoch 126, Average Loss: 0.7400, Accuracy: 0.5800\n",
            "==============================\n",
            "TRAIN: Epoch 127, Average Loss: 0.4462, Accuracy: 0.8080\n",
            "TEST: Epoch 127, Average Loss: 0.7353, Accuracy: 0.5825\n",
            "==============================\n",
            "TRAIN: Epoch 128, Average Loss: 0.4701, Accuracy: 0.7980\n",
            "TEST: Epoch 128, Average Loss: 0.7337, Accuracy: 0.5856\n",
            "==============================\n",
            "TRAIN: Epoch 129, Average Loss: 0.4535, Accuracy: 0.8180\n",
            "TEST: Epoch 129, Average Loss: 0.7382, Accuracy: 0.6005\n",
            "==============================\n",
            "TRAIN: Epoch 130, Average Loss: 0.4502, Accuracy: 0.7905\n",
            "TEST: Epoch 130, Average Loss: 0.7436, Accuracy: 0.5688\n",
            "==============================\n",
            "TRAIN: Epoch 131, Average Loss: 0.4718, Accuracy: 0.7731\n",
            "TEST: Epoch 131, Average Loss: 0.7375, Accuracy: 0.5986\n",
            "==============================\n",
            "TRAIN: Epoch 132, Average Loss: 0.4645, Accuracy: 0.7905\n",
            "TEST: Epoch 132, Average Loss: 0.7337, Accuracy: 0.5999\n",
            "==============================\n",
            "TRAIN: Epoch 133, Average Loss: 0.4643, Accuracy: 0.7855\n",
            "TEST: Epoch 133, Average Loss: 0.7390, Accuracy: 0.5781\n",
            "==============================\n",
            "TRAIN: Epoch 134, Average Loss: 0.4604, Accuracy: 0.8005\n",
            "TEST: Epoch 134, Average Loss: 0.7400, Accuracy: 0.6005\n",
            "==============================\n",
            "TRAIN: Epoch 135, Average Loss: 0.4483, Accuracy: 0.8030\n",
            "TEST: Epoch 135, Average Loss: 0.7510, Accuracy: 0.5650\n",
            "==============================\n",
            "TRAIN: Epoch 136, Average Loss: 0.4357, Accuracy: 0.8329\n",
            "TEST: Epoch 136, Average Loss: 0.7465, Accuracy: 0.5893\n",
            "==============================\n",
            "TRAIN: Epoch 137, Average Loss: 0.4505, Accuracy: 0.8055\n",
            "TEST: Epoch 137, Average Loss: 0.7495, Accuracy: 0.5806\n",
            "==============================\n",
            "TRAIN: Epoch 138, Average Loss: 0.4606, Accuracy: 0.7955\n",
            "TEST: Epoch 138, Average Loss: 0.7406, Accuracy: 0.5993\n",
            "==============================\n",
            "TRAIN: Epoch 139, Average Loss: 0.4418, Accuracy: 0.8204\n",
            "TEST: Epoch 139, Average Loss: 0.7494, Accuracy: 0.5737\n",
            "==============================\n",
            "TRAIN: Epoch 140, Average Loss: 0.4447, Accuracy: 0.8155\n",
            "TEST: Epoch 140, Average Loss: 0.7520, Accuracy: 0.5818\n",
            "==============================\n",
            "TRAIN: Epoch 141, Average Loss: 0.4605, Accuracy: 0.7905\n",
            "TEST: Epoch 141, Average Loss: 0.7455, Accuracy: 0.5874\n",
            "==============================\n",
            "TRAIN: Epoch 142, Average Loss: 0.4423, Accuracy: 0.7955\n",
            "TEST: Epoch 142, Average Loss: 0.7523, Accuracy: 0.5856\n",
            "==============================\n",
            "TRAIN: Epoch 143, Average Loss: 0.4432, Accuracy: 0.8005\n",
            "TEST: Epoch 143, Average Loss: 0.7624, Accuracy: 0.5874\n",
            "==============================\n",
            "TRAIN: Epoch 144, Average Loss: 0.4323, Accuracy: 0.7855\n",
            "TEST: Epoch 144, Average Loss: 0.7487, Accuracy: 0.5918\n",
            "==============================\n",
            "TRAIN: Epoch 145, Average Loss: 0.4399, Accuracy: 0.8329\n",
            "TEST: Epoch 145, Average Loss: 0.7595, Accuracy: 0.5775\n",
            "==============================\n",
            "TRAIN: Epoch 146, Average Loss: 0.4421, Accuracy: 0.8030\n",
            "TEST: Epoch 146, Average Loss: 0.7683, Accuracy: 0.5706\n",
            "==============================\n",
            "TRAIN: Epoch 147, Average Loss: 0.4370, Accuracy: 0.7980\n",
            "TEST: Epoch 147, Average Loss: 0.7556, Accuracy: 0.5968\n",
            "==============================\n",
            "TRAIN: Epoch 148, Average Loss: 0.4428, Accuracy: 0.8080\n",
            "TEST: Epoch 148, Average Loss: 0.7716, Accuracy: 0.5731\n",
            "==============================\n",
            "TRAIN: Epoch 149, Average Loss: 0.4440, Accuracy: 0.8155\n",
            "TEST: Epoch 149, Average Loss: 0.7613, Accuracy: 0.5762\n",
            "==============================\n",
            "TRAIN: Epoch 150, Average Loss: 0.4401, Accuracy: 0.7980\n",
            "TEST: Epoch 150, Average Loss: 0.7623, Accuracy: 0.5862\n",
            "==============================\n",
            "TRAIN: Epoch 151, Average Loss: 0.4537, Accuracy: 0.8030\n",
            "TEST: Epoch 151, Average Loss: 0.7612, Accuracy: 0.5688\n",
            "==============================\n",
            "TRAIN: Epoch 152, Average Loss: 0.4270, Accuracy: 0.8254\n",
            "TEST: Epoch 152, Average Loss: 0.7589, Accuracy: 0.5825\n",
            "==============================\n",
            "TRAIN: Epoch 153, Average Loss: 0.4264, Accuracy: 0.8080\n",
            "TEST: Epoch 153, Average Loss: 0.7604, Accuracy: 0.5769\n",
            "==============================\n",
            "TRAIN: Epoch 154, Average Loss: 0.4176, Accuracy: 0.8379\n",
            "TEST: Epoch 154, Average Loss: 0.7655, Accuracy: 0.5713\n",
            "==============================\n",
            "TRAIN: Epoch 155, Average Loss: 0.4390, Accuracy: 0.8329\n",
            "TEST: Epoch 155, Average Loss: 0.7673, Accuracy: 0.5644\n",
            "==============================\n",
            "TRAIN: Epoch 156, Average Loss: 0.4455, Accuracy: 0.8030\n",
            "TEST: Epoch 156, Average Loss: 0.7648, Accuracy: 0.5924\n",
            "==============================\n",
            "TRAIN: Epoch 157, Average Loss: 0.4242, Accuracy: 0.8254\n",
            "TEST: Epoch 157, Average Loss: 0.7771, Accuracy: 0.5700\n",
            "==============================\n",
            "TRAIN: Epoch 158, Average Loss: 0.4100, Accuracy: 0.8304\n",
            "TEST: Epoch 158, Average Loss: 0.7796, Accuracy: 0.5775\n",
            "==============================\n",
            "TRAIN: Epoch 159, Average Loss: 0.4429, Accuracy: 0.8080\n",
            "TEST: Epoch 159, Average Loss: 0.7725, Accuracy: 0.5762\n",
            "==============================\n",
            "TRAIN: Epoch 160, Average Loss: 0.4343, Accuracy: 0.8155\n",
            "TEST: Epoch 160, Average Loss: 0.7743, Accuracy: 0.5781\n",
            "==============================\n",
            "TRAIN: Epoch 161, Average Loss: 0.4246, Accuracy: 0.8180\n",
            "TEST: Epoch 161, Average Loss: 0.7771, Accuracy: 0.5781\n",
            "==============================\n",
            "TRAIN: Epoch 162, Average Loss: 0.4216, Accuracy: 0.8204\n",
            "TEST: Epoch 162, Average Loss: 0.7751, Accuracy: 0.5775\n",
            "==============================\n",
            "TRAIN: Epoch 163, Average Loss: 0.4301, Accuracy: 0.8155\n",
            "TEST: Epoch 163, Average Loss: 0.7709, Accuracy: 0.5781\n",
            "==============================\n",
            "TRAIN: Epoch 164, Average Loss: 0.3947, Accuracy: 0.8354\n",
            "TEST: Epoch 164, Average Loss: 0.7753, Accuracy: 0.5781\n",
            "==============================\n",
            "TRAIN: Epoch 165, Average Loss: 0.4224, Accuracy: 0.8130\n",
            "TEST: Epoch 165, Average Loss: 0.7777, Accuracy: 0.5793\n",
            "==============================\n",
            "TRAIN: Epoch 166, Average Loss: 0.4057, Accuracy: 0.8279\n",
            "TEST: Epoch 166, Average Loss: 0.7754, Accuracy: 0.5862\n",
            "==============================\n",
            "TRAIN: Epoch 167, Average Loss: 0.4267, Accuracy: 0.8055\n",
            "TEST: Epoch 167, Average Loss: 0.7747, Accuracy: 0.5781\n",
            "==============================\n",
            "TRAIN: Epoch 168, Average Loss: 0.4100, Accuracy: 0.8329\n",
            "TEST: Epoch 168, Average Loss: 0.7819, Accuracy: 0.5818\n",
            "==============================\n",
            "TRAIN: Epoch 169, Average Loss: 0.4333, Accuracy: 0.7930\n",
            "TEST: Epoch 169, Average Loss: 0.7824, Accuracy: 0.5862\n",
            "==============================\n",
            "TRAIN: Epoch 170, Average Loss: 0.4126, Accuracy: 0.8254\n",
            "TEST: Epoch 170, Average Loss: 0.7842, Accuracy: 0.5737\n",
            "==============================\n",
            "TRAIN: Epoch 171, Average Loss: 0.4000, Accuracy: 0.8354\n",
            "TEST: Epoch 171, Average Loss: 0.7817, Accuracy: 0.5706\n",
            "==============================\n",
            "TRAIN: Epoch 172, Average Loss: 0.4173, Accuracy: 0.8304\n",
            "TEST: Epoch 172, Average Loss: 0.7907, Accuracy: 0.5849\n",
            "==============================\n",
            "TRAIN: Epoch 173, Average Loss: 0.4133, Accuracy: 0.8279\n",
            "TEST: Epoch 173, Average Loss: 0.7940, Accuracy: 0.5744\n",
            "==============================\n",
            "TRAIN: Epoch 174, Average Loss: 0.4051, Accuracy: 0.8404\n",
            "TEST: Epoch 174, Average Loss: 0.7868, Accuracy: 0.5762\n",
            "==============================\n",
            "TRAIN: Epoch 175, Average Loss: 0.4000, Accuracy: 0.8304\n",
            "TEST: Epoch 175, Average Loss: 0.7890, Accuracy: 0.5893\n",
            "==============================\n",
            "TRAIN: Epoch 176, Average Loss: 0.4147, Accuracy: 0.8204\n",
            "TEST: Epoch 176, Average Loss: 0.8025, Accuracy: 0.5625\n",
            "==============================\n",
            "TRAIN: Epoch 177, Average Loss: 0.4071, Accuracy: 0.8229\n",
            "TEST: Epoch 177, Average Loss: 0.7930, Accuracy: 0.5856\n",
            "==============================\n",
            "TRAIN: Epoch 178, Average Loss: 0.4192, Accuracy: 0.8130\n",
            "TEST: Epoch 178, Average Loss: 0.7950, Accuracy: 0.5719\n",
            "==============================\n",
            "TRAIN: Epoch 179, Average Loss: 0.4194, Accuracy: 0.8204\n",
            "TEST: Epoch 179, Average Loss: 0.7970, Accuracy: 0.5843\n",
            "==============================\n",
            "TRAIN: Epoch 180, Average Loss: 0.4268, Accuracy: 0.8304\n",
            "TEST: Epoch 180, Average Loss: 0.7950, Accuracy: 0.5825\n",
            "==============================\n",
            "TRAIN: Epoch 181, Average Loss: 0.3966, Accuracy: 0.8279\n",
            "TEST: Epoch 181, Average Loss: 0.7897, Accuracy: 0.5825\n",
            "==============================\n",
            "TRAIN: Epoch 182, Average Loss: 0.4132, Accuracy: 0.8404\n",
            "TEST: Epoch 182, Average Loss: 0.7960, Accuracy: 0.5775\n",
            "==============================\n",
            "TRAIN: Epoch 183, Average Loss: 0.3956, Accuracy: 0.8204\n",
            "TEST: Epoch 183, Average Loss: 0.7945, Accuracy: 0.5862\n",
            "==============================\n",
            "TRAIN: Epoch 184, Average Loss: 0.3876, Accuracy: 0.8379\n",
            "TEST: Epoch 184, Average Loss: 0.8005, Accuracy: 0.5825\n",
            "==============================\n",
            "TRAIN: Epoch 185, Average Loss: 0.4270, Accuracy: 0.8204\n",
            "TEST: Epoch 185, Average Loss: 0.8015, Accuracy: 0.5874\n",
            "==============================\n",
            "TRAIN: Epoch 186, Average Loss: 0.3938, Accuracy: 0.8354\n",
            "TEST: Epoch 186, Average Loss: 0.7999, Accuracy: 0.5769\n",
            "==============================\n",
            "TRAIN: Epoch 187, Average Loss: 0.4121, Accuracy: 0.8279\n",
            "TEST: Epoch 187, Average Loss: 0.7987, Accuracy: 0.5899\n",
            "==============================\n",
            "TRAIN: Epoch 188, Average Loss: 0.4034, Accuracy: 0.8354\n",
            "TEST: Epoch 188, Average Loss: 0.8061, Accuracy: 0.5650\n",
            "==============================\n",
            "TRAIN: Epoch 189, Average Loss: 0.3965, Accuracy: 0.8080\n",
            "TEST: Epoch 189, Average Loss: 0.8073, Accuracy: 0.5781\n",
            "==============================\n",
            "TRAIN: Epoch 190, Average Loss: 0.3960, Accuracy: 0.8030\n",
            "TEST: Epoch 190, Average Loss: 0.7976, Accuracy: 0.5825\n",
            "==============================\n",
            "TRAIN: Epoch 191, Average Loss: 0.4164, Accuracy: 0.8254\n",
            "TEST: Epoch 191, Average Loss: 0.8090, Accuracy: 0.5744\n",
            "==============================\n",
            "TRAIN: Epoch 192, Average Loss: 0.3979, Accuracy: 0.8204\n",
            "TEST: Epoch 192, Average Loss: 0.7988, Accuracy: 0.5912\n",
            "==============================\n",
            "TRAIN: Epoch 193, Average Loss: 0.4049, Accuracy: 0.8229\n",
            "TEST: Epoch 193, Average Loss: 0.8082, Accuracy: 0.5675\n",
            "==============================\n",
            "TRAIN: Epoch 194, Average Loss: 0.4099, Accuracy: 0.8105\n",
            "TEST: Epoch 194, Average Loss: 0.8098, Accuracy: 0.5868\n",
            "==============================\n",
            "TRAIN: Epoch 195, Average Loss: 0.4091, Accuracy: 0.8229\n",
            "TEST: Epoch 195, Average Loss: 0.8076, Accuracy: 0.5800\n",
            "==============================\n",
            "TRAIN: Epoch 196, Average Loss: 0.3970, Accuracy: 0.8429\n",
            "TEST: Epoch 196, Average Loss: 0.8088, Accuracy: 0.5793\n",
            "==============================\n",
            "TRAIN: Epoch 197, Average Loss: 0.4148, Accuracy: 0.8279\n",
            "TEST: Epoch 197, Average Loss: 0.8112, Accuracy: 0.5856\n",
            "==============================\n",
            "TRAIN: Epoch 198, Average Loss: 0.3784, Accuracy: 0.8254\n",
            "TEST: Epoch 198, Average Loss: 0.8075, Accuracy: 0.5781\n",
            "==============================\n",
            "TRAIN: Epoch 199, Average Loss: 0.3851, Accuracy: 0.8304\n",
            "TEST: Epoch 199, Average Loss: 0.8076, Accuracy: 0.5843\n",
            "==============================\n",
            "TRAIN: Epoch 200, Average Loss: 0.3898, Accuracy: 0.8254\n",
            "TEST: Epoch 200, Average Loss: 0.8110, Accuracy: 0.5812\n",
            "==============================\n",
            "TRAIN: Epoch 201, Average Loss: 0.4058, Accuracy: 0.8229\n",
            "TEST: Epoch 201, Average Loss: 0.8050, Accuracy: 0.5781\n",
            "==============================\n",
            "TRAIN: Epoch 202, Average Loss: 0.3771, Accuracy: 0.8504\n",
            "TEST: Epoch 202, Average Loss: 0.8147, Accuracy: 0.5818\n",
            "==============================\n",
            "TRAIN: Epoch 203, Average Loss: 0.3787, Accuracy: 0.8429\n",
            "TEST: Epoch 203, Average Loss: 0.8232, Accuracy: 0.5831\n",
            "==============================\n",
            "TRAIN: Epoch 204, Average Loss: 0.4233, Accuracy: 0.7930\n",
            "TEST: Epoch 204, Average Loss: 0.8107, Accuracy: 0.5831\n",
            "==============================\n",
            "TRAIN: Epoch 205, Average Loss: 0.3674, Accuracy: 0.8529\n",
            "TEST: Epoch 205, Average Loss: 0.8234, Accuracy: 0.5632\n",
            "==============================\n",
            "TRAIN: Epoch 206, Average Loss: 0.3718, Accuracy: 0.8554\n",
            "TEST: Epoch 206, Average Loss: 0.8232, Accuracy: 0.5831\n",
            "==============================\n",
            "TRAIN: Epoch 207, Average Loss: 0.3809, Accuracy: 0.8304\n",
            "TEST: Epoch 207, Average Loss: 0.8177, Accuracy: 0.5806\n",
            "==============================\n",
            "TRAIN: Epoch 208, Average Loss: 0.3726, Accuracy: 0.8753\n",
            "TEST: Epoch 208, Average Loss: 0.8207, Accuracy: 0.5825\n",
            "==============================\n",
            "TRAIN: Epoch 209, Average Loss: 0.3985, Accuracy: 0.8229\n",
            "TEST: Epoch 209, Average Loss: 0.8187, Accuracy: 0.5775\n",
            "==============================\n",
            "TRAIN: Epoch 210, Average Loss: 0.3873, Accuracy: 0.8304\n",
            "TEST: Epoch 210, Average Loss: 0.8207, Accuracy: 0.5825\n",
            "==============================\n",
            "TRAIN: Epoch 211, Average Loss: 0.3929, Accuracy: 0.8479\n",
            "TEST: Epoch 211, Average Loss: 0.8197, Accuracy: 0.5800\n",
            "==============================\n",
            "TRAIN: Epoch 212, Average Loss: 0.3916, Accuracy: 0.8354\n",
            "TEST: Epoch 212, Average Loss: 0.8188, Accuracy: 0.5787\n",
            "==============================\n",
            "TRAIN: Epoch 213, Average Loss: 0.3933, Accuracy: 0.8304\n",
            "TEST: Epoch 213, Average Loss: 0.8207, Accuracy: 0.5731\n",
            "==============================\n",
            "TRAIN: Epoch 214, Average Loss: 0.3755, Accuracy: 0.8628\n",
            "TEST: Epoch 214, Average Loss: 0.8238, Accuracy: 0.5800\n",
            "==============================\n",
            "TRAIN: Epoch 215, Average Loss: 0.3897, Accuracy: 0.8279\n",
            "TEST: Epoch 215, Average Loss: 0.8301, Accuracy: 0.5800\n",
            "==============================\n",
            "TRAIN: Epoch 216, Average Loss: 0.3856, Accuracy: 0.8479\n",
            "TEST: Epoch 216, Average Loss: 0.8199, Accuracy: 0.5750\n",
            "==============================\n",
            "TRAIN: Epoch 217, Average Loss: 0.3638, Accuracy: 0.8628\n",
            "TEST: Epoch 217, Average Loss: 0.8255, Accuracy: 0.5806\n",
            "==============================\n",
            "TRAIN: Epoch 218, Average Loss: 0.3843, Accuracy: 0.8304\n",
            "TEST: Epoch 218, Average Loss: 0.8372, Accuracy: 0.5868\n",
            "==============================\n",
            "TRAIN: Epoch 219, Average Loss: 0.3687, Accuracy: 0.8454\n",
            "TEST: Epoch 219, Average Loss: 0.8304, Accuracy: 0.5576\n",
            "==============================\n",
            "TRAIN: Epoch 220, Average Loss: 0.3859, Accuracy: 0.8429\n",
            "TEST: Epoch 220, Average Loss: 0.8266, Accuracy: 0.5924\n",
            "==============================\n",
            "TRAIN: Epoch 221, Average Loss: 0.3811, Accuracy: 0.8254\n",
            "TEST: Epoch 221, Average Loss: 0.8293, Accuracy: 0.5787\n",
            "==============================\n",
            "TRAIN: Epoch 222, Average Loss: 0.3867, Accuracy: 0.8304\n",
            "TEST: Epoch 222, Average Loss: 0.8336, Accuracy: 0.5762\n",
            "==============================\n",
            "TRAIN: Epoch 223, Average Loss: 0.3823, Accuracy: 0.8554\n",
            "TEST: Epoch 223, Average Loss: 0.8322, Accuracy: 0.5781\n",
            "==============================\n",
            "TRAIN: Epoch 224, Average Loss: 0.3638, Accuracy: 0.8504\n",
            "TEST: Epoch 224, Average Loss: 0.8312, Accuracy: 0.5793\n",
            "==============================\n",
            "TRAIN: Epoch 225, Average Loss: 0.3742, Accuracy: 0.8678\n",
            "TEST: Epoch 225, Average Loss: 0.8371, Accuracy: 0.5706\n",
            "==============================\n",
            "TRAIN: Epoch 226, Average Loss: 0.3603, Accuracy: 0.8504\n",
            "TEST: Epoch 226, Average Loss: 0.8401, Accuracy: 0.5800\n",
            "==============================\n",
            "TRAIN: Epoch 227, Average Loss: 0.3765, Accuracy: 0.8454\n",
            "TEST: Epoch 227, Average Loss: 0.8378, Accuracy: 0.5793\n",
            "==============================\n",
            "TRAIN: Epoch 228, Average Loss: 0.3796, Accuracy: 0.8454\n",
            "TEST: Epoch 228, Average Loss: 0.8361, Accuracy: 0.5775\n",
            "==============================\n",
            "TRAIN: Epoch 229, Average Loss: 0.3791, Accuracy: 0.8504\n",
            "TEST: Epoch 229, Average Loss: 0.8440, Accuracy: 0.5769\n",
            "==============================\n",
            "TRAIN: Epoch 230, Average Loss: 0.3910, Accuracy: 0.8279\n",
            "TEST: Epoch 230, Average Loss: 0.8383, Accuracy: 0.5725\n",
            "==============================\n",
            "TRAIN: Epoch 231, Average Loss: 0.3669, Accuracy: 0.8404\n",
            "TEST: Epoch 231, Average Loss: 0.8395, Accuracy: 0.5806\n",
            "==============================\n",
            "TRAIN: Epoch 232, Average Loss: 0.3720, Accuracy: 0.8529\n",
            "TEST: Epoch 232, Average Loss: 0.8360, Accuracy: 0.5818\n",
            "==============================\n",
            "TRAIN: Epoch 233, Average Loss: 0.3434, Accuracy: 0.8628\n",
            "TEST: Epoch 233, Average Loss: 0.8457, Accuracy: 0.5750\n",
            "==============================\n",
            "TRAIN: Epoch 234, Average Loss: 0.3856, Accuracy: 0.8603\n",
            "TEST: Epoch 234, Average Loss: 0.8479, Accuracy: 0.5825\n",
            "==============================\n",
            "TRAIN: Epoch 235, Average Loss: 0.3700, Accuracy: 0.8554\n",
            "TEST: Epoch 235, Average Loss: 0.8585, Accuracy: 0.5731\n",
            "==============================\n",
            "TRAIN: Epoch 236, Average Loss: 0.3717, Accuracy: 0.8479\n",
            "TEST: Epoch 236, Average Loss: 0.8555, Accuracy: 0.5762\n",
            "==============================\n",
            "TRAIN: Epoch 237, Average Loss: 0.3837, Accuracy: 0.8279\n",
            "TEST: Epoch 237, Average Loss: 0.8465, Accuracy: 0.5837\n",
            "==============================\n",
            "TRAIN: Epoch 238, Average Loss: 0.3634, Accuracy: 0.8703\n",
            "TEST: Epoch 238, Average Loss: 0.8647, Accuracy: 0.5576\n",
            "==============================\n",
            "TRAIN: Epoch 239, Average Loss: 0.3808, Accuracy: 0.8379\n",
            "TEST: Epoch 239, Average Loss: 0.8486, Accuracy: 0.5762\n",
            "==============================\n",
            "TRAIN: Epoch 240, Average Loss: 0.3460, Accuracy: 0.8703\n",
            "TEST: Epoch 240, Average Loss: 0.8577, Accuracy: 0.5638\n",
            "==============================\n",
            "TRAIN: Epoch 241, Average Loss: 0.3829, Accuracy: 0.8279\n",
            "TEST: Epoch 241, Average Loss: 0.8522, Accuracy: 0.5787\n",
            "==============================\n",
            "TRAIN: Epoch 242, Average Loss: 0.3408, Accuracy: 0.8653\n",
            "TEST: Epoch 242, Average Loss: 0.8559, Accuracy: 0.5632\n",
            "==============================\n",
            "TRAIN: Epoch 243, Average Loss: 0.3509, Accuracy: 0.8678\n",
            "TEST: Epoch 243, Average Loss: 0.8572, Accuracy: 0.5650\n",
            "==============================\n",
            "TRAIN: Epoch 244, Average Loss: 0.3541, Accuracy: 0.8628\n",
            "TEST: Epoch 244, Average Loss: 0.8599, Accuracy: 0.5806\n",
            "==============================\n",
            "TRAIN: Epoch 245, Average Loss: 0.3483, Accuracy: 0.8778\n",
            "TEST: Epoch 245, Average Loss: 0.8498, Accuracy: 0.5725\n",
            "==============================\n",
            "TRAIN: Epoch 246, Average Loss: 0.3447, Accuracy: 0.8529\n",
            "TEST: Epoch 246, Average Loss: 0.8498, Accuracy: 0.5800\n",
            "==============================\n",
            "TRAIN: Epoch 247, Average Loss: 0.3431, Accuracy: 0.8603\n",
            "TEST: Epoch 247, Average Loss: 0.8524, Accuracy: 0.5713\n",
            "==============================\n",
            "TRAIN: Epoch 248, Average Loss: 0.3588, Accuracy: 0.8554\n",
            "TEST: Epoch 248, Average Loss: 0.8600, Accuracy: 0.5775\n",
            "==============================\n",
            "TRAIN: Epoch 249, Average Loss: 0.3494, Accuracy: 0.8653\n",
            "TEST: Epoch 249, Average Loss: 0.8730, Accuracy: 0.5663\n",
            "==============================\n",
            "TRAIN: Epoch 250, Average Loss: 0.3749, Accuracy: 0.8579\n",
            "TEST: Epoch 250, Average Loss: 0.8654, Accuracy: 0.5688\n",
            "==============================\n",
            "TRAIN: Epoch 251, Average Loss: 0.3623, Accuracy: 0.8454\n",
            "TEST: Epoch 251, Average Loss: 0.8624, Accuracy: 0.5862\n",
            "==============================\n",
            "TRAIN: Epoch 252, Average Loss: 0.3583, Accuracy: 0.8603\n",
            "TEST: Epoch 252, Average Loss: 0.8609, Accuracy: 0.5675\n",
            "==============================\n",
            "TRAIN: Epoch 253, Average Loss: 0.3441, Accuracy: 0.8703\n",
            "TEST: Epoch 253, Average Loss: 0.8720, Accuracy: 0.5744\n",
            "==============================\n",
            "TRAIN: Epoch 254, Average Loss: 0.3784, Accuracy: 0.8329\n",
            "TEST: Epoch 254, Average Loss: 0.8631, Accuracy: 0.5713\n",
            "==============================\n",
            "TRAIN: Epoch 255, Average Loss: 0.3274, Accuracy: 0.8803\n",
            "TEST: Epoch 255, Average Loss: 0.8687, Accuracy: 0.5644\n",
            "==============================\n",
            "TRAIN: Epoch 256, Average Loss: 0.3583, Accuracy: 0.8504\n",
            "TEST: Epoch 256, Average Loss: 0.8639, Accuracy: 0.5713\n",
            "==============================\n",
            "TRAIN: Epoch 257, Average Loss: 0.3527, Accuracy: 0.8603\n",
            "TEST: Epoch 257, Average Loss: 0.8679, Accuracy: 0.5688\n",
            "==============================\n",
            "TRAIN: Epoch 258, Average Loss: 0.3441, Accuracy: 0.8828\n",
            "TEST: Epoch 258, Average Loss: 0.8683, Accuracy: 0.5769\n",
            "==============================\n",
            "TRAIN: Epoch 259, Average Loss: 0.3426, Accuracy: 0.8628\n",
            "TEST: Epoch 259, Average Loss: 0.8681, Accuracy: 0.5756\n",
            "==============================\n",
            "TRAIN: Epoch 260, Average Loss: 0.3542, Accuracy: 0.8628\n",
            "TEST: Epoch 260, Average Loss: 0.8661, Accuracy: 0.5706\n",
            "==============================\n",
            "TRAIN: Epoch 261, Average Loss: 0.3417, Accuracy: 0.8678\n",
            "TEST: Epoch 261, Average Loss: 0.8678, Accuracy: 0.5831\n",
            "==============================\n",
            "TRAIN: Epoch 262, Average Loss: 0.3536, Accuracy: 0.8504\n",
            "TEST: Epoch 262, Average Loss: 0.8786, Accuracy: 0.5594\n",
            "==============================\n",
            "TRAIN: Epoch 263, Average Loss: 0.3529, Accuracy: 0.8504\n",
            "TEST: Epoch 263, Average Loss: 0.8760, Accuracy: 0.5582\n",
            "==============================\n",
            "TRAIN: Epoch 264, Average Loss: 0.3713, Accuracy: 0.8603\n",
            "TEST: Epoch 264, Average Loss: 0.8714, Accuracy: 0.5825\n",
            "==============================\n",
            "TRAIN: Epoch 265, Average Loss: 0.3540, Accuracy: 0.8454\n",
            "TEST: Epoch 265, Average Loss: 0.8695, Accuracy: 0.5650\n",
            "==============================\n",
            "TRAIN: Epoch 266, Average Loss: 0.3443, Accuracy: 0.8554\n",
            "TEST: Epoch 266, Average Loss: 0.8715, Accuracy: 0.5744\n",
            "==============================\n",
            "TRAIN: Epoch 267, Average Loss: 0.3520, Accuracy: 0.8579\n",
            "TEST: Epoch 267, Average Loss: 0.8827, Accuracy: 0.5706\n",
            "==============================\n",
            "TRAIN: Epoch 268, Average Loss: 0.3332, Accuracy: 0.8728\n",
            "TEST: Epoch 268, Average Loss: 0.8712, Accuracy: 0.5625\n",
            "==============================\n",
            "TRAIN: Epoch 269, Average Loss: 0.3315, Accuracy: 0.8628\n",
            "TEST: Epoch 269, Average Loss: 0.8711, Accuracy: 0.5762\n",
            "==============================\n",
            "TRAIN: Epoch 270, Average Loss: 0.3660, Accuracy: 0.8603\n",
            "TEST: Epoch 270, Average Loss: 0.8882, Accuracy: 0.5663\n",
            "==============================\n",
            "TRAIN: Epoch 271, Average Loss: 0.3110, Accuracy: 0.8828\n",
            "TEST: Epoch 271, Average Loss: 0.8879, Accuracy: 0.5657\n",
            "==============================\n",
            "TRAIN: Epoch 272, Average Loss: 0.3478, Accuracy: 0.8603\n",
            "TEST: Epoch 272, Average Loss: 0.8804, Accuracy: 0.5750\n",
            "==============================\n",
            "TRAIN: Epoch 273, Average Loss: 0.3525, Accuracy: 0.8529\n",
            "TEST: Epoch 273, Average Loss: 0.8776, Accuracy: 0.5737\n",
            "==============================\n",
            "TRAIN: Epoch 274, Average Loss: 0.3309, Accuracy: 0.8678\n",
            "TEST: Epoch 274, Average Loss: 0.8804, Accuracy: 0.5582\n",
            "==============================\n",
            "TRAIN: Epoch 275, Average Loss: 0.3240, Accuracy: 0.8828\n",
            "TEST: Epoch 275, Average Loss: 0.8773, Accuracy: 0.5681\n",
            "==============================\n",
            "TRAIN: Epoch 276, Average Loss: 0.3498, Accuracy: 0.8554\n",
            "TEST: Epoch 276, Average Loss: 0.8866, Accuracy: 0.5800\n",
            "==============================\n",
            "TRAIN: Epoch 277, Average Loss: 0.3286, Accuracy: 0.8803\n",
            "TEST: Epoch 277, Average Loss: 0.8818, Accuracy: 0.5750\n",
            "==============================\n",
            "TRAIN: Epoch 278, Average Loss: 0.3291, Accuracy: 0.8853\n",
            "TEST: Epoch 278, Average Loss: 0.8884, Accuracy: 0.5569\n",
            "==============================\n",
            "TRAIN: Epoch 279, Average Loss: 0.3187, Accuracy: 0.8853\n",
            "TEST: Epoch 279, Average Loss: 0.8960, Accuracy: 0.5638\n",
            "==============================\n",
            "TRAIN: Epoch 280, Average Loss: 0.3313, Accuracy: 0.8903\n",
            "TEST: Epoch 280, Average Loss: 0.8894, Accuracy: 0.5688\n",
            "==============================\n",
            "TRAIN: Epoch 281, Average Loss: 0.3241, Accuracy: 0.8728\n",
            "TEST: Epoch 281, Average Loss: 0.8948, Accuracy: 0.5607\n",
            "==============================\n",
            "TRAIN: Epoch 282, Average Loss: 0.3434, Accuracy: 0.8603\n",
            "TEST: Epoch 282, Average Loss: 0.8898, Accuracy: 0.5588\n",
            "==============================\n",
            "TRAIN: Epoch 283, Average Loss: 0.3412, Accuracy: 0.8703\n",
            "TEST: Epoch 283, Average Loss: 0.9019, Accuracy: 0.5600\n",
            "==============================\n",
            "TRAIN: Epoch 284, Average Loss: 0.3449, Accuracy: 0.8678\n",
            "TEST: Epoch 284, Average Loss: 0.8929, Accuracy: 0.5600\n",
            "==============================\n",
            "TRAIN: Epoch 285, Average Loss: 0.3329, Accuracy: 0.8653\n",
            "TEST: Epoch 285, Average Loss: 0.8991, Accuracy: 0.5638\n",
            "==============================\n",
            "TRAIN: Epoch 286, Average Loss: 0.3438, Accuracy: 0.8529\n",
            "TEST: Epoch 286, Average Loss: 0.8985, Accuracy: 0.5657\n",
            "==============================\n",
            "TRAIN: Epoch 287, Average Loss: 0.3251, Accuracy: 0.8753\n",
            "TEST: Epoch 287, Average Loss: 0.9033, Accuracy: 0.5657\n",
            "==============================\n",
            "TRAIN: Epoch 288, Average Loss: 0.3346, Accuracy: 0.8778\n",
            "TEST: Epoch 288, Average Loss: 0.9008, Accuracy: 0.5507\n",
            "==============================\n",
            "TRAIN: Epoch 289, Average Loss: 0.3132, Accuracy: 0.8828\n",
            "TEST: Epoch 289, Average Loss: 0.9148, Accuracy: 0.5762\n",
            "==============================\n",
            "TRAIN: Epoch 290, Average Loss: 0.3282, Accuracy: 0.8678\n",
            "TEST: Epoch 290, Average Loss: 0.9055, Accuracy: 0.5638\n",
            "==============================\n",
            "TRAIN: Epoch 291, Average Loss: 0.3306, Accuracy: 0.8853\n",
            "TEST: Epoch 291, Average Loss: 0.9060, Accuracy: 0.5781\n",
            "==============================\n",
            "TRAIN: Epoch 292, Average Loss: 0.3454, Accuracy: 0.8554\n",
            "TEST: Epoch 292, Average Loss: 0.9058, Accuracy: 0.5607\n",
            "==============================\n",
            "TRAIN: Epoch 293, Average Loss: 0.3236, Accuracy: 0.8853\n",
            "TEST: Epoch 293, Average Loss: 0.8938, Accuracy: 0.5725\n",
            "==============================\n",
            "TRAIN: Epoch 294, Average Loss: 0.3156, Accuracy: 0.8903\n",
            "TEST: Epoch 294, Average Loss: 0.9019, Accuracy: 0.5501\n",
            "==============================\n",
            "TRAIN: Epoch 295, Average Loss: 0.3278, Accuracy: 0.8803\n",
            "TEST: Epoch 295, Average Loss: 0.9081, Accuracy: 0.5831\n",
            "==============================\n",
            "TRAIN: Epoch 296, Average Loss: 0.3300, Accuracy: 0.8728\n",
            "TEST: Epoch 296, Average Loss: 0.9154, Accuracy: 0.5426\n",
            "==============================\n",
            "TRAIN: Epoch 297, Average Loss: 0.3216, Accuracy: 0.8803\n",
            "TEST: Epoch 297, Average Loss: 0.9173, Accuracy: 0.5688\n",
            "==============================\n",
            "TRAIN: Epoch 298, Average Loss: 0.3274, Accuracy: 0.8778\n",
            "TEST: Epoch 298, Average Loss: 0.9126, Accuracy: 0.5594\n",
            "==============================\n",
            "TRAIN: Epoch 299, Average Loss: 0.3156, Accuracy: 0.8928\n",
            "TEST: Epoch 299, Average Loss: 0.9167, Accuracy: 0.5576\n",
            "==============================\n",
            "TRAIN: Epoch 300, Average Loss: 0.3158, Accuracy: 0.8853\n",
            "TEST: Epoch 300, Average Loss: 0.9115, Accuracy: 0.5613\n",
            "==============================\n",
            "TRAIN: Epoch 301, Average Loss: 0.3432, Accuracy: 0.8554\n",
            "TEST: Epoch 301, Average Loss: 0.9084, Accuracy: 0.5756\n",
            "==============================\n",
            "TRAIN: Epoch 302, Average Loss: 0.3392, Accuracy: 0.8803\n",
            "TEST: Epoch 302, Average Loss: 0.9087, Accuracy: 0.5632\n",
            "==============================\n",
            "TRAIN: Epoch 303, Average Loss: 0.3322, Accuracy: 0.8479\n",
            "TEST: Epoch 303, Average Loss: 0.9071, Accuracy: 0.5713\n",
            "==============================\n",
            "TRAIN: Epoch 304, Average Loss: 0.3134, Accuracy: 0.8928\n",
            "TEST: Epoch 304, Average Loss: 0.9036, Accuracy: 0.5675\n",
            "==============================\n",
            "TRAIN: Epoch 305, Average Loss: 0.3326, Accuracy: 0.8628\n",
            "TEST: Epoch 305, Average Loss: 0.9107, Accuracy: 0.5769\n",
            "==============================\n",
            "TRAIN: Epoch 306, Average Loss: 0.3062, Accuracy: 0.8953\n",
            "TEST: Epoch 306, Average Loss: 0.9095, Accuracy: 0.5476\n",
            "==============================\n",
            "TRAIN: Epoch 307, Average Loss: 0.3321, Accuracy: 0.8753\n",
            "TEST: Epoch 307, Average Loss: 0.9205, Accuracy: 0.5607\n",
            "==============================\n",
            "TRAIN: Epoch 308, Average Loss: 0.3391, Accuracy: 0.8728\n",
            "TEST: Epoch 308, Average Loss: 0.9154, Accuracy: 0.5731\n",
            "==============================\n",
            "TRAIN: Epoch 309, Average Loss: 0.3238, Accuracy: 0.8678\n",
            "TEST: Epoch 309, Average Loss: 0.9223, Accuracy: 0.5632\n",
            "==============================\n",
            "TRAIN: Epoch 310, Average Loss: 0.3168, Accuracy: 0.8753\n",
            "TEST: Epoch 310, Average Loss: 0.9162, Accuracy: 0.5650\n",
            "==============================\n",
            "TRAIN: Epoch 311, Average Loss: 0.3227, Accuracy: 0.8703\n",
            "TEST: Epoch 311, Average Loss: 0.9270, Accuracy: 0.5806\n",
            "==============================\n",
            "TRAIN: Epoch 312, Average Loss: 0.3208, Accuracy: 0.8653\n",
            "TEST: Epoch 312, Average Loss: 0.9161, Accuracy: 0.5632\n",
            "==============================\n",
            "TRAIN: Epoch 313, Average Loss: 0.3067, Accuracy: 0.8903\n",
            "TEST: Epoch 313, Average Loss: 0.9191, Accuracy: 0.5706\n",
            "==============================\n",
            "TRAIN: Epoch 314, Average Loss: 0.3097, Accuracy: 0.8628\n",
            "TEST: Epoch 314, Average Loss: 0.9158, Accuracy: 0.5831\n",
            "==============================\n",
            "TRAIN: Epoch 315, Average Loss: 0.3018, Accuracy: 0.8928\n",
            "TEST: Epoch 315, Average Loss: 0.9184, Accuracy: 0.5588\n",
            "==============================\n",
            "TRAIN: Epoch 316, Average Loss: 0.3128, Accuracy: 0.8778\n",
            "TEST: Epoch 316, Average Loss: 0.9250, Accuracy: 0.5663\n",
            "==============================\n",
            "TRAIN: Epoch 317, Average Loss: 0.3052, Accuracy: 0.8903\n",
            "TEST: Epoch 317, Average Loss: 0.9293, Accuracy: 0.5713\n",
            "==============================\n",
            "TRAIN: Epoch 318, Average Loss: 0.3100, Accuracy: 0.8878\n",
            "TEST: Epoch 318, Average Loss: 0.9314, Accuracy: 0.5526\n",
            "==============================\n",
            "TRAIN: Epoch 319, Average Loss: 0.3186, Accuracy: 0.8828\n",
            "TEST: Epoch 319, Average Loss: 0.9232, Accuracy: 0.5644\n",
            "==============================\n",
            "TRAIN: Epoch 320, Average Loss: 0.3020, Accuracy: 0.8928\n",
            "TEST: Epoch 320, Average Loss: 0.9380, Accuracy: 0.5657\n",
            "==============================\n",
            "TRAIN: Epoch 321, Average Loss: 0.3048, Accuracy: 0.8753\n",
            "TEST: Epoch 321, Average Loss: 0.9313, Accuracy: 0.5775\n",
            "==============================\n",
            "TRAIN: Epoch 322, Average Loss: 0.3208, Accuracy: 0.8753\n",
            "TEST: Epoch 322, Average Loss: 0.9436, Accuracy: 0.5607\n",
            "==============================\n",
            "TRAIN: Epoch 323, Average Loss: 0.2959, Accuracy: 0.8803\n",
            "TEST: Epoch 323, Average Loss: 0.9571, Accuracy: 0.5744\n",
            "==============================\n",
            "TRAIN: Epoch 324, Average Loss: 0.3232, Accuracy: 0.8703\n",
            "TEST: Epoch 324, Average Loss: 0.9335, Accuracy: 0.5756\n",
            "==============================\n",
            "TRAIN: Epoch 325, Average Loss: 0.3305, Accuracy: 0.8579\n",
            "TEST: Epoch 325, Average Loss: 0.9439, Accuracy: 0.5520\n",
            "==============================\n",
            "TRAIN: Epoch 326, Average Loss: 0.3089, Accuracy: 0.8828\n",
            "TEST: Epoch 326, Average Loss: 0.9429, Accuracy: 0.5874\n",
            "==============================\n",
            "TRAIN: Epoch 327, Average Loss: 0.3174, Accuracy: 0.8678\n",
            "TEST: Epoch 327, Average Loss: 0.9441, Accuracy: 0.5588\n",
            "==============================\n",
            "TRAIN: Epoch 328, Average Loss: 0.3037, Accuracy: 0.8978\n",
            "TEST: Epoch 328, Average Loss: 0.9318, Accuracy: 0.5526\n",
            "==============================\n",
            "TRAIN: Epoch 329, Average Loss: 0.3055, Accuracy: 0.8828\n",
            "TEST: Epoch 329, Average Loss: 0.9385, Accuracy: 0.5775\n",
            "==============================\n",
            "TRAIN: Epoch 330, Average Loss: 0.3156, Accuracy: 0.8678\n",
            "TEST: Epoch 330, Average Loss: 0.9358, Accuracy: 0.5725\n",
            "==============================\n",
            "TRAIN: Epoch 331, Average Loss: 0.2979, Accuracy: 0.9002\n",
            "TEST: Epoch 331, Average Loss: 0.9448, Accuracy: 0.5476\n",
            "==============================\n",
            "TRAIN: Epoch 332, Average Loss: 0.3139, Accuracy: 0.8703\n",
            "TEST: Epoch 332, Average Loss: 0.9631, Accuracy: 0.5831\n",
            "==============================\n",
            "TRAIN: Epoch 333, Average Loss: 0.3131, Accuracy: 0.8554\n",
            "TEST: Epoch 333, Average Loss: 0.9392, Accuracy: 0.5607\n",
            "==============================\n",
            "TRAIN: Epoch 334, Average Loss: 0.2891, Accuracy: 0.9027\n",
            "TEST: Epoch 334, Average Loss: 0.9448, Accuracy: 0.5520\n",
            "==============================\n",
            "TRAIN: Epoch 335, Average Loss: 0.3228, Accuracy: 0.8753\n",
            "TEST: Epoch 335, Average Loss: 0.9493, Accuracy: 0.5756\n",
            "==============================\n",
            "TRAIN: Epoch 336, Average Loss: 0.3184, Accuracy: 0.8853\n",
            "TEST: Epoch 336, Average Loss: 0.9557, Accuracy: 0.5476\n",
            "==============================\n",
            "TRAIN: Epoch 337, Average Loss: 0.3036, Accuracy: 0.9002\n",
            "TEST: Epoch 337, Average Loss: 0.9483, Accuracy: 0.5694\n",
            "==============================\n",
            "TRAIN: Epoch 338, Average Loss: 0.3089, Accuracy: 0.8853\n",
            "TEST: Epoch 338, Average Loss: 0.9551, Accuracy: 0.5520\n",
            "==============================\n",
            "TRAIN: Epoch 339, Average Loss: 0.2938, Accuracy: 0.8903\n",
            "TEST: Epoch 339, Average Loss: 0.9423, Accuracy: 0.5495\n",
            "==============================\n",
            "TRAIN: Epoch 340, Average Loss: 0.3034, Accuracy: 0.8728\n",
            "TEST: Epoch 340, Average Loss: 0.9534, Accuracy: 0.5638\n",
            "==============================\n",
            "TRAIN: Epoch 341, Average Loss: 0.3075, Accuracy: 0.8728\n",
            "TEST: Epoch 341, Average Loss: 0.9400, Accuracy: 0.5470\n",
            "==============================\n",
            "TRAIN: Epoch 342, Average Loss: 0.3065, Accuracy: 0.8803\n",
            "TEST: Epoch 342, Average Loss: 0.9653, Accuracy: 0.5713\n",
            "==============================\n",
            "TRAIN: Epoch 343, Average Loss: 0.3255, Accuracy: 0.8678\n",
            "TEST: Epoch 343, Average Loss: 0.9474, Accuracy: 0.5569\n",
            "==============================\n",
            "TRAIN: Epoch 344, Average Loss: 0.3009, Accuracy: 0.8803\n",
            "TEST: Epoch 344, Average Loss: 0.9495, Accuracy: 0.5588\n",
            "==============================\n",
            "TRAIN: Epoch 345, Average Loss: 0.3233, Accuracy: 0.8803\n",
            "TEST: Epoch 345, Average Loss: 0.9573, Accuracy: 0.5657\n",
            "==============================\n",
            "TRAIN: Epoch 346, Average Loss: 0.2840, Accuracy: 0.8903\n",
            "TEST: Epoch 346, Average Loss: 0.9576, Accuracy: 0.5569\n",
            "==============================\n",
            "TRAIN: Epoch 347, Average Loss: 0.3115, Accuracy: 0.8603\n",
            "TEST: Epoch 347, Average Loss: 0.9729, Accuracy: 0.5538\n",
            "==============================\n",
            "TRAIN: Epoch 348, Average Loss: 0.3024, Accuracy: 0.8803\n",
            "TEST: Epoch 348, Average Loss: 0.9586, Accuracy: 0.5532\n",
            "==============================\n",
            "TRAIN: Epoch 349, Average Loss: 0.3003, Accuracy: 0.9027\n",
            "TEST: Epoch 349, Average Loss: 0.9596, Accuracy: 0.5544\n",
            "==============================\n",
            "TRAIN: Epoch 350, Average Loss: 0.2989, Accuracy: 0.8653\n",
            "TEST: Epoch 350, Average Loss: 0.9600, Accuracy: 0.5756\n",
            "==============================\n",
            "TRAIN: Epoch 351, Average Loss: 0.3175, Accuracy: 0.8728\n",
            "TEST: Epoch 351, Average Loss: 0.9623, Accuracy: 0.5383\n",
            "==============================\n",
            "TRAIN: Epoch 352, Average Loss: 0.3099, Accuracy: 0.8928\n",
            "TEST: Epoch 352, Average Loss: 0.9769, Accuracy: 0.5613\n",
            "==============================\n",
            "TRAIN: Epoch 353, Average Loss: 0.2870, Accuracy: 0.8953\n",
            "TEST: Epoch 353, Average Loss: 0.9736, Accuracy: 0.5924\n",
            "==============================\n",
            "TRAIN: Epoch 354, Average Loss: 0.2957, Accuracy: 0.8853\n",
            "TEST: Epoch 354, Average Loss: 0.9593, Accuracy: 0.5451\n",
            "==============================\n",
            "TRAIN: Epoch 355, Average Loss: 0.3177, Accuracy: 0.8753\n",
            "TEST: Epoch 355, Average Loss: 0.9690, Accuracy: 0.5470\n",
            "==============================\n",
            "TRAIN: Epoch 356, Average Loss: 0.3079, Accuracy: 0.8953\n",
            "TEST: Epoch 356, Average Loss: 0.9682, Accuracy: 0.5700\n",
            "==============================\n",
            "TRAIN: Epoch 357, Average Loss: 0.3016, Accuracy: 0.8678\n",
            "TEST: Epoch 357, Average Loss: 0.9707, Accuracy: 0.5563\n",
            "==============================\n",
            "TRAIN: Epoch 358, Average Loss: 0.2875, Accuracy: 0.8853\n",
            "TEST: Epoch 358, Average Loss: 0.9546, Accuracy: 0.5594\n",
            "==============================\n",
            "TRAIN: Epoch 359, Average Loss: 0.2964, Accuracy: 0.8778\n",
            "TEST: Epoch 359, Average Loss: 0.9783, Accuracy: 0.5451\n",
            "==============================\n",
            "TRAIN: Epoch 360, Average Loss: 0.3093, Accuracy: 0.8828\n",
            "TEST: Epoch 360, Average Loss: 0.9538, Accuracy: 0.5663\n",
            "==============================\n",
            "TRAIN: Epoch 361, Average Loss: 0.3105, Accuracy: 0.9027\n",
            "TEST: Epoch 361, Average Loss: 0.9571, Accuracy: 0.5812\n",
            "==============================\n",
            "TRAIN: Epoch 362, Average Loss: 0.2902, Accuracy: 0.9002\n",
            "TEST: Epoch 362, Average Loss: 0.9604, Accuracy: 0.5464\n",
            "==============================\n",
            "TRAIN: Epoch 363, Average Loss: 0.3148, Accuracy: 0.8728\n",
            "TEST: Epoch 363, Average Loss: 0.9816, Accuracy: 0.5638\n",
            "==============================\n",
            "TRAIN: Epoch 364, Average Loss: 0.2960, Accuracy: 0.8878\n",
            "TEST: Epoch 364, Average Loss: 0.9647, Accuracy: 0.5588\n",
            "==============================\n",
            "TRAIN: Epoch 365, Average Loss: 0.3019, Accuracy: 0.8778\n",
            "TEST: Epoch 365, Average Loss: 0.9745, Accuracy: 0.5594\n",
            "==============================\n",
            "TRAIN: Epoch 366, Average Loss: 0.2907, Accuracy: 0.8803\n",
            "TEST: Epoch 366, Average Loss: 0.9702, Accuracy: 0.5588\n",
            "==============================\n",
            "TRAIN: Epoch 367, Average Loss: 0.3048, Accuracy: 0.8703\n",
            "TEST: Epoch 367, Average Loss: 0.9702, Accuracy: 0.5414\n",
            "==============================\n",
            "TRAIN: Epoch 368, Average Loss: 0.2767, Accuracy: 0.8978\n",
            "TEST: Epoch 368, Average Loss: 0.9903, Accuracy: 0.5532\n",
            "==============================\n",
            "TRAIN: Epoch 369, Average Loss: 0.2966, Accuracy: 0.8803\n",
            "TEST: Epoch 369, Average Loss: 0.9708, Accuracy: 0.5588\n",
            "==============================\n",
            "TRAIN: Epoch 370, Average Loss: 0.3091, Accuracy: 0.8753\n",
            "TEST: Epoch 370, Average Loss: 0.9762, Accuracy: 0.5600\n",
            "==============================\n",
            "TRAIN: Epoch 371, Average Loss: 0.3040, Accuracy: 0.8928\n",
            "TEST: Epoch 371, Average Loss: 0.9717, Accuracy: 0.5694\n",
            "==============================\n",
            "TRAIN: Epoch 372, Average Loss: 0.2962, Accuracy: 0.8853\n",
            "TEST: Epoch 372, Average Loss: 0.9739, Accuracy: 0.5657\n",
            "==============================\n",
            "TRAIN: Epoch 373, Average Loss: 0.3091, Accuracy: 0.8803\n",
            "TEST: Epoch 373, Average Loss: 0.9718, Accuracy: 0.5532\n",
            "==============================\n",
            "TRAIN: Epoch 374, Average Loss: 0.2831, Accuracy: 0.8928\n",
            "TEST: Epoch 374, Average Loss: 0.9800, Accuracy: 0.5650\n",
            "==============================\n",
            "TRAIN: Epoch 375, Average Loss: 0.2784, Accuracy: 0.8828\n",
            "TEST: Epoch 375, Average Loss: 0.9682, Accuracy: 0.5719\n",
            "==============================\n",
            "TRAIN: Epoch 376, Average Loss: 0.2739, Accuracy: 0.9102\n",
            "TEST: Epoch 376, Average Loss: 0.9695, Accuracy: 0.5576\n",
            "==============================\n",
            "TRAIN: Epoch 377, Average Loss: 0.2859, Accuracy: 0.9027\n",
            "TEST: Epoch 377, Average Loss: 0.9855, Accuracy: 0.5607\n",
            "==============================\n",
            "TRAIN: Epoch 378, Average Loss: 0.2823, Accuracy: 0.8878\n",
            "TEST: Epoch 378, Average Loss: 0.9937, Accuracy: 0.5551\n",
            "==============================\n",
            "TRAIN: Epoch 379, Average Loss: 0.3118, Accuracy: 0.8828\n",
            "TEST: Epoch 379, Average Loss: 0.9746, Accuracy: 0.5625\n",
            "==============================\n",
            "TRAIN: Epoch 380, Average Loss: 0.2952, Accuracy: 0.8953\n",
            "TEST: Epoch 380, Average Loss: 0.9781, Accuracy: 0.5681\n",
            "==============================\n",
            "TRAIN: Epoch 381, Average Loss: 0.3056, Accuracy: 0.8878\n",
            "TEST: Epoch 381, Average Loss: 0.9795, Accuracy: 0.5451\n",
            "==============================\n",
            "TRAIN: Epoch 382, Average Loss: 0.3085, Accuracy: 0.8653\n",
            "TEST: Epoch 382, Average Loss: 0.9816, Accuracy: 0.5700\n",
            "==============================\n",
            "TRAIN: Epoch 383, Average Loss: 0.2896, Accuracy: 0.8778\n",
            "TEST: Epoch 383, Average Loss: 0.9759, Accuracy: 0.5532\n",
            "==============================\n",
            "TRAIN: Epoch 384, Average Loss: 0.2819, Accuracy: 0.8853\n",
            "TEST: Epoch 384, Average Loss: 0.9898, Accuracy: 0.5476\n",
            "==============================\n",
            "TRAIN: Epoch 385, Average Loss: 0.2747, Accuracy: 0.9102\n",
            "TEST: Epoch 385, Average Loss: 0.9883, Accuracy: 0.5694\n",
            "==============================\n",
            "TRAIN: Epoch 386, Average Loss: 0.2817, Accuracy: 0.8828\n",
            "TEST: Epoch 386, Average Loss: 0.9996, Accuracy: 0.5644\n",
            "==============================\n",
            "TRAIN: Epoch 387, Average Loss: 0.3052, Accuracy: 0.8628\n",
            "TEST: Epoch 387, Average Loss: 0.9874, Accuracy: 0.5557\n",
            "==============================\n",
            "TRAIN: Epoch 388, Average Loss: 0.2827, Accuracy: 0.8903\n",
            "TEST: Epoch 388, Average Loss: 0.9907, Accuracy: 0.5663\n",
            "==============================\n",
            "TRAIN: Epoch 389, Average Loss: 0.2825, Accuracy: 0.8878\n",
            "TEST: Epoch 389, Average Loss: 1.0006, Accuracy: 0.5675\n",
            "==============================\n",
            "TRAIN: Epoch 390, Average Loss: 0.2859, Accuracy: 0.9002\n",
            "TEST: Epoch 390, Average Loss: 0.9901, Accuracy: 0.5650\n",
            "==============================\n",
            "TRAIN: Epoch 391, Average Loss: 0.2903, Accuracy: 0.8928\n",
            "TEST: Epoch 391, Average Loss: 0.9865, Accuracy: 0.5632\n",
            "==============================\n",
            "TRAIN: Epoch 392, Average Loss: 0.2799, Accuracy: 0.9027\n",
            "TEST: Epoch 392, Average Loss: 0.9944, Accuracy: 0.5625\n",
            "==============================\n",
            "TRAIN: Epoch 393, Average Loss: 0.2723, Accuracy: 0.8978\n",
            "TEST: Epoch 393, Average Loss: 0.9877, Accuracy: 0.5582\n",
            "==============================\n",
            "TRAIN: Epoch 394, Average Loss: 0.2652, Accuracy: 0.9027\n",
            "TEST: Epoch 394, Average Loss: 1.0124, Accuracy: 0.5513\n",
            "==============================\n",
            "TRAIN: Epoch 395, Average Loss: 0.3054, Accuracy: 0.8703\n",
            "TEST: Epoch 395, Average Loss: 0.9922, Accuracy: 0.5588\n",
            "==============================\n",
            "TRAIN: Epoch 396, Average Loss: 0.2972, Accuracy: 0.9052\n",
            "TEST: Epoch 396, Average Loss: 1.0080, Accuracy: 0.5563\n",
            "==============================\n",
            "TRAIN: Epoch 397, Average Loss: 0.3009, Accuracy: 0.8903\n",
            "TEST: Epoch 397, Average Loss: 0.9924, Accuracy: 0.5625\n",
            "==============================\n",
            "TRAIN: Epoch 398, Average Loss: 0.3029, Accuracy: 0.8603\n",
            "TEST: Epoch 398, Average Loss: 0.9881, Accuracy: 0.5488\n",
            "==============================\n",
            "TRAIN: Epoch 399, Average Loss: 0.2534, Accuracy: 0.9102\n",
            "TEST: Epoch 399, Average Loss: 0.9934, Accuracy: 0.5538\n",
            "==============================\n",
            "TRAIN: Epoch 400, Average Loss: 0.2692, Accuracy: 0.8928\n",
            "TEST: Epoch 400, Average Loss: 0.9928, Accuracy: 0.5470\n",
            "==============================\n",
            "TRAIN: Epoch 401, Average Loss: 0.2717, Accuracy: 0.8953\n",
            "TEST: Epoch 401, Average Loss: 1.0139, Accuracy: 0.5544\n",
            "==============================\n",
            "TRAIN: Epoch 402, Average Loss: 0.2935, Accuracy: 0.8878\n",
            "TEST: Epoch 402, Average Loss: 1.0135, Accuracy: 0.5507\n",
            "==============================\n",
            "TRAIN: Epoch 403, Average Loss: 0.2677, Accuracy: 0.9102\n",
            "TEST: Epoch 403, Average Loss: 1.0110, Accuracy: 0.5520\n",
            "==============================\n",
            "TRAIN: Epoch 404, Average Loss: 0.2935, Accuracy: 0.9052\n",
            "TEST: Epoch 404, Average Loss: 1.0171, Accuracy: 0.5507\n",
            "==============================\n",
            "TRAIN: Epoch 405, Average Loss: 0.2746, Accuracy: 0.9027\n",
            "TEST: Epoch 405, Average Loss: 1.0183, Accuracy: 0.5563\n",
            "==============================\n",
            "TRAIN: Epoch 406, Average Loss: 0.2909, Accuracy: 0.8753\n",
            "TEST: Epoch 406, Average Loss: 1.0026, Accuracy: 0.5600\n",
            "==============================\n",
            "TRAIN: Epoch 407, Average Loss: 0.2653, Accuracy: 0.9102\n",
            "TEST: Epoch 407, Average Loss: 1.0004, Accuracy: 0.5588\n",
            "==============================\n",
            "TRAIN: Epoch 408, Average Loss: 0.2623, Accuracy: 0.9102\n",
            "TEST: Epoch 408, Average Loss: 1.0191, Accuracy: 0.5594\n",
            "==============================\n",
            "TRAIN: Epoch 409, Average Loss: 0.2705, Accuracy: 0.8903\n",
            "TEST: Epoch 409, Average Loss: 1.0094, Accuracy: 0.5495\n",
            "==============================\n",
            "TRAIN: Epoch 410, Average Loss: 0.2586, Accuracy: 0.9102\n",
            "TEST: Epoch 410, Average Loss: 1.0253, Accuracy: 0.5756\n",
            "==============================\n",
            "TRAIN: Epoch 411, Average Loss: 0.2620, Accuracy: 0.9052\n",
            "TEST: Epoch 411, Average Loss: 1.0075, Accuracy: 0.5576\n",
            "==============================\n",
            "TRAIN: Epoch 412, Average Loss: 0.2770, Accuracy: 0.8978\n",
            "TEST: Epoch 412, Average Loss: 1.0046, Accuracy: 0.5464\n",
            "==============================\n",
            "TRAIN: Epoch 413, Average Loss: 0.2893, Accuracy: 0.8778\n",
            "TEST: Epoch 413, Average Loss: 1.0102, Accuracy: 0.5600\n",
            "==============================\n",
            "TRAIN: Epoch 414, Average Loss: 0.2594, Accuracy: 0.9077\n",
            "TEST: Epoch 414, Average Loss: 1.0059, Accuracy: 0.5694\n",
            "==============================\n",
            "TRAIN: Epoch 415, Average Loss: 0.2738, Accuracy: 0.9077\n",
            "TEST: Epoch 415, Average Loss: 1.0106, Accuracy: 0.5594\n",
            "==============================\n",
            "TRAIN: Epoch 416, Average Loss: 0.2934, Accuracy: 0.8828\n",
            "TEST: Epoch 416, Average Loss: 1.0152, Accuracy: 0.5544\n",
            "==============================\n",
            "TRAIN: Epoch 417, Average Loss: 0.2545, Accuracy: 0.9152\n",
            "TEST: Epoch 417, Average Loss: 1.0225, Accuracy: 0.5694\n",
            "==============================\n",
            "TRAIN: Epoch 418, Average Loss: 0.2793, Accuracy: 0.8903\n",
            "TEST: Epoch 418, Average Loss: 1.0226, Accuracy: 0.5526\n",
            "==============================\n",
            "TRAIN: Epoch 419, Average Loss: 0.2776, Accuracy: 0.9052\n",
            "TEST: Epoch 419, Average Loss: 1.0146, Accuracy: 0.5507\n",
            "==============================\n",
            "TRAIN: Epoch 420, Average Loss: 0.2681, Accuracy: 0.8978\n",
            "TEST: Epoch 420, Average Loss: 1.0097, Accuracy: 0.5681\n",
            "==============================\n",
            "TRAIN: Epoch 421, Average Loss: 0.2608, Accuracy: 0.8928\n",
            "TEST: Epoch 421, Average Loss: 1.0129, Accuracy: 0.5457\n",
            "==============================\n",
            "TRAIN: Epoch 422, Average Loss: 0.2897, Accuracy: 0.8953\n",
            "TEST: Epoch 422, Average Loss: 1.0286, Accuracy: 0.5600\n",
            "==============================\n",
            "TRAIN: Epoch 423, Average Loss: 0.2789, Accuracy: 0.8928\n",
            "TEST: Epoch 423, Average Loss: 1.0172, Accuracy: 0.5619\n",
            "==============================\n",
            "TRAIN: Epoch 424, Average Loss: 0.2721, Accuracy: 0.8853\n",
            "TEST: Epoch 424, Average Loss: 1.0178, Accuracy: 0.5594\n",
            "==============================\n",
            "TRAIN: Epoch 425, Average Loss: 0.3012, Accuracy: 0.8878\n",
            "TEST: Epoch 425, Average Loss: 1.0391, Accuracy: 0.5389\n",
            "==============================\n",
            "TRAIN: Epoch 426, Average Loss: 0.2711, Accuracy: 0.8853\n",
            "TEST: Epoch 426, Average Loss: 1.0263, Accuracy: 0.5744\n",
            "==============================\n",
            "TRAIN: Epoch 427, Average Loss: 0.2515, Accuracy: 0.9127\n",
            "TEST: Epoch 427, Average Loss: 1.0329, Accuracy: 0.5464\n",
            "==============================\n",
            "TRAIN: Epoch 428, Average Loss: 0.2827, Accuracy: 0.9102\n",
            "TEST: Epoch 428, Average Loss: 1.0347, Accuracy: 0.5488\n",
            "==============================\n",
            "TRAIN: Epoch 429, Average Loss: 0.2604, Accuracy: 0.9027\n",
            "TEST: Epoch 429, Average Loss: 1.0331, Accuracy: 0.5607\n",
            "==============================\n",
            "TRAIN: Epoch 430, Average Loss: 0.2648, Accuracy: 0.9002\n",
            "TEST: Epoch 430, Average Loss: 1.0405, Accuracy: 0.5345\n",
            "==============================\n",
            "TRAIN: Epoch 431, Average Loss: 0.2779, Accuracy: 0.8878\n",
            "TEST: Epoch 431, Average Loss: 1.0392, Accuracy: 0.5501\n",
            "==============================\n",
            "TRAIN: Epoch 432, Average Loss: 0.3114, Accuracy: 0.8878\n",
            "TEST: Epoch 432, Average Loss: 1.0242, Accuracy: 0.5482\n",
            "==============================\n",
            "TRAIN: Epoch 433, Average Loss: 0.2526, Accuracy: 0.9077\n",
            "TEST: Epoch 433, Average Loss: 1.0492, Accuracy: 0.5501\n",
            "==============================\n",
            "TRAIN: Epoch 434, Average Loss: 0.2586, Accuracy: 0.9177\n",
            "TEST: Epoch 434, Average Loss: 1.0330, Accuracy: 0.5482\n",
            "==============================\n",
            "TRAIN: Epoch 435, Average Loss: 0.2581, Accuracy: 0.9127\n",
            "TEST: Epoch 435, Average Loss: 1.0426, Accuracy: 0.5569\n",
            "==============================\n",
            "TRAIN: Epoch 436, Average Loss: 0.2808, Accuracy: 0.9027\n",
            "TEST: Epoch 436, Average Loss: 1.0210, Accuracy: 0.5501\n",
            "==============================\n",
            "TRAIN: Epoch 437, Average Loss: 0.2631, Accuracy: 0.8978\n",
            "TEST: Epoch 437, Average Loss: 1.0443, Accuracy: 0.5607\n",
            "==============================\n",
            "TRAIN: Epoch 438, Average Loss: 0.2599, Accuracy: 0.8928\n",
            "TEST: Epoch 438, Average Loss: 1.0529, Accuracy: 0.5395\n",
            "==============================\n",
            "TRAIN: Epoch 439, Average Loss: 0.2437, Accuracy: 0.9077\n",
            "TEST: Epoch 439, Average Loss: 1.0651, Accuracy: 0.5625\n",
            "==============================\n",
            "TRAIN: Epoch 440, Average Loss: 0.2841, Accuracy: 0.8928\n",
            "TEST: Epoch 440, Average Loss: 1.0425, Accuracy: 0.5464\n",
            "==============================\n",
            "TRAIN: Epoch 441, Average Loss: 0.2916, Accuracy: 0.8953\n",
            "TEST: Epoch 441, Average Loss: 1.0484, Accuracy: 0.5576\n",
            "==============================\n",
            "TRAIN: Epoch 442, Average Loss: 0.2413, Accuracy: 0.9177\n",
            "TEST: Epoch 442, Average Loss: 1.0492, Accuracy: 0.5725\n",
            "==============================\n",
            "TRAIN: Epoch 443, Average Loss: 0.2563, Accuracy: 0.8953\n",
            "TEST: Epoch 443, Average Loss: 1.0423, Accuracy: 0.5414\n",
            "==============================\n",
            "TRAIN: Epoch 444, Average Loss: 0.3020, Accuracy: 0.8803\n",
            "TEST: Epoch 444, Average Loss: 1.0289, Accuracy: 0.5470\n",
            "==============================\n",
            "TRAIN: Epoch 445, Average Loss: 0.2478, Accuracy: 0.9102\n",
            "TEST: Epoch 445, Average Loss: 1.0447, Accuracy: 0.5457\n",
            "==============================\n",
            "TRAIN: Epoch 446, Average Loss: 0.2457, Accuracy: 0.9077\n",
            "TEST: Epoch 446, Average Loss: 1.0524, Accuracy: 0.5476\n",
            "==============================\n",
            "TRAIN: Epoch 447, Average Loss: 0.2762, Accuracy: 0.8878\n",
            "TEST: Epoch 447, Average Loss: 1.0512, Accuracy: 0.5476\n",
            "==============================\n",
            "TRAIN: Epoch 448, Average Loss: 0.2845, Accuracy: 0.9027\n",
            "TEST: Epoch 448, Average Loss: 1.0426, Accuracy: 0.5420\n",
            "==============================\n",
            "TRAIN: Epoch 449, Average Loss: 0.2700, Accuracy: 0.9177\n",
            "TEST: Epoch 449, Average Loss: 1.0489, Accuracy: 0.5482\n",
            "==============================\n",
            "TRAIN: Epoch 450, Average Loss: 0.2534, Accuracy: 0.9327\n",
            "TEST: Epoch 450, Average Loss: 1.0462, Accuracy: 0.5737\n",
            "==============================\n",
            "TRAIN: Epoch 451, Average Loss: 0.2440, Accuracy: 0.8978\n",
            "TEST: Epoch 451, Average Loss: 1.0475, Accuracy: 0.5526\n",
            "==============================\n",
            "TRAIN: Epoch 452, Average Loss: 0.2866, Accuracy: 0.8978\n",
            "TEST: Epoch 452, Average Loss: 1.0454, Accuracy: 0.5451\n",
            "==============================\n",
            "TRAIN: Epoch 453, Average Loss: 0.2933, Accuracy: 0.9002\n",
            "TEST: Epoch 453, Average Loss: 1.0496, Accuracy: 0.5414\n",
            "==============================\n",
            "TRAIN: Epoch 454, Average Loss: 0.2863, Accuracy: 0.8728\n",
            "TEST: Epoch 454, Average Loss: 1.0532, Accuracy: 0.5607\n",
            "==============================\n",
            "TRAIN: Epoch 455, Average Loss: 0.2577, Accuracy: 0.8978\n",
            "TEST: Epoch 455, Average Loss: 1.0521, Accuracy: 0.5414\n",
            "==============================\n",
            "TRAIN: Epoch 456, Average Loss: 0.2584, Accuracy: 0.8978\n",
            "TEST: Epoch 456, Average Loss: 1.0405, Accuracy: 0.5495\n",
            "==============================\n",
            "TRAIN: Epoch 457, Average Loss: 0.2508, Accuracy: 0.9152\n",
            "TEST: Epoch 457, Average Loss: 1.0432, Accuracy: 0.5395\n",
            "==============================\n",
            "TRAIN: Epoch 458, Average Loss: 0.2718, Accuracy: 0.9077\n",
            "TEST: Epoch 458, Average Loss: 1.0639, Accuracy: 0.5538\n",
            "==============================\n",
            "TRAIN: Epoch 459, Average Loss: 0.3072, Accuracy: 0.8703\n",
            "TEST: Epoch 459, Average Loss: 1.0685, Accuracy: 0.5694\n",
            "==============================\n",
            "TRAIN: Epoch 460, Average Loss: 0.2638, Accuracy: 0.9052\n",
            "TEST: Epoch 460, Average Loss: 1.0492, Accuracy: 0.5632\n",
            "==============================\n",
            "TRAIN: Epoch 461, Average Loss: 0.2711, Accuracy: 0.9027\n",
            "TEST: Epoch 461, Average Loss: 1.0450, Accuracy: 0.5638\n",
            "==============================\n",
            "TRAIN: Epoch 462, Average Loss: 0.2722, Accuracy: 0.9002\n",
            "TEST: Epoch 462, Average Loss: 1.0535, Accuracy: 0.5569\n",
            "==============================\n",
            "TRAIN: Epoch 463, Average Loss: 0.2583, Accuracy: 0.9177\n",
            "TEST: Epoch 463, Average Loss: 1.0492, Accuracy: 0.5495\n",
            "==============================\n",
            "TRAIN: Epoch 464, Average Loss: 0.2526, Accuracy: 0.9027\n",
            "TEST: Epoch 464, Average Loss: 1.0521, Accuracy: 0.5507\n",
            "==============================\n",
            "TRAIN: Epoch 465, Average Loss: 0.2345, Accuracy: 0.9127\n",
            "TEST: Epoch 465, Average Loss: 1.0626, Accuracy: 0.5526\n",
            "==============================\n",
            "TRAIN: Epoch 466, Average Loss: 0.2560, Accuracy: 0.9077\n",
            "TEST: Epoch 466, Average Loss: 1.0650, Accuracy: 0.5526\n",
            "==============================\n",
            "TRAIN: Epoch 467, Average Loss: 0.2577, Accuracy: 0.8928\n",
            "TEST: Epoch 467, Average Loss: 1.0566, Accuracy: 0.5464\n",
            "==============================\n",
            "TRAIN: Epoch 468, Average Loss: 0.2627, Accuracy: 0.9052\n",
            "TEST: Epoch 468, Average Loss: 1.0683, Accuracy: 0.5476\n",
            "==============================\n",
            "TRAIN: Epoch 469, Average Loss: 0.2616, Accuracy: 0.8803\n",
            "TEST: Epoch 469, Average Loss: 1.0671, Accuracy: 0.5657\n",
            "==============================\n",
            "TRAIN: Epoch 470, Average Loss: 0.2725, Accuracy: 0.8953\n",
            "TEST: Epoch 470, Average Loss: 1.0632, Accuracy: 0.5576\n",
            "==============================\n",
            "TRAIN: Epoch 471, Average Loss: 0.2493, Accuracy: 0.9177\n",
            "TEST: Epoch 471, Average Loss: 1.0610, Accuracy: 0.5432\n",
            "==============================\n",
            "TRAIN: Epoch 472, Average Loss: 0.2653, Accuracy: 0.8853\n",
            "TEST: Epoch 472, Average Loss: 1.0494, Accuracy: 0.5426\n",
            "==============================\n",
            "TRAIN: Epoch 473, Average Loss: 0.2567, Accuracy: 0.8978\n",
            "TEST: Epoch 473, Average Loss: 1.0610, Accuracy: 0.5650\n",
            "==============================\n",
            "TRAIN: Epoch 474, Average Loss: 0.2735, Accuracy: 0.8953\n",
            "TEST: Epoch 474, Average Loss: 1.0600, Accuracy: 0.5495\n",
            "==============================\n",
            "TRAIN: Epoch 475, Average Loss: 0.2462, Accuracy: 0.9052\n",
            "TEST: Epoch 475, Average Loss: 1.0641, Accuracy: 0.5420\n",
            "==============================\n",
            "TRAIN: Epoch 476, Average Loss: 0.2672, Accuracy: 0.8778\n",
            "TEST: Epoch 476, Average Loss: 1.0633, Accuracy: 0.5775\n",
            "==============================\n",
            "TRAIN: Epoch 477, Average Loss: 0.2750, Accuracy: 0.8853\n",
            "TEST: Epoch 477, Average Loss: 1.0668, Accuracy: 0.5495\n",
            "==============================\n",
            "TRAIN: Epoch 478, Average Loss: 0.2509, Accuracy: 0.9027\n",
            "TEST: Epoch 478, Average Loss: 1.0610, Accuracy: 0.5588\n",
            "==============================\n",
            "TRAIN: Epoch 479, Average Loss: 0.2383, Accuracy: 0.9227\n",
            "TEST: Epoch 479, Average Loss: 1.0578, Accuracy: 0.5426\n",
            "==============================\n",
            "TRAIN: Epoch 480, Average Loss: 0.2423, Accuracy: 0.9052\n",
            "TEST: Epoch 480, Average Loss: 1.0918, Accuracy: 0.5482\n",
            "==============================\n",
            "TRAIN: Epoch 481, Average Loss: 0.2751, Accuracy: 0.8828\n",
            "TEST: Epoch 481, Average Loss: 1.0763, Accuracy: 0.5563\n",
            "==============================\n",
            "TRAIN: Epoch 482, Average Loss: 0.2347, Accuracy: 0.9202\n",
            "TEST: Epoch 482, Average Loss: 1.0698, Accuracy: 0.5569\n",
            "==============================\n",
            "TRAIN: Epoch 483, Average Loss: 0.2650, Accuracy: 0.8953\n",
            "TEST: Epoch 483, Average Loss: 1.0736, Accuracy: 0.5551\n",
            "==============================\n",
            "TRAIN: Epoch 484, Average Loss: 0.2659, Accuracy: 0.9027\n",
            "TEST: Epoch 484, Average Loss: 1.0609, Accuracy: 0.5501\n",
            "==============================\n",
            "TRAIN: Epoch 485, Average Loss: 0.2391, Accuracy: 0.9127\n",
            "TEST: Epoch 485, Average Loss: 1.1143, Accuracy: 0.5563\n",
            "==============================\n",
            "TRAIN: Epoch 486, Average Loss: 0.2540, Accuracy: 0.8928\n",
            "TEST: Epoch 486, Average Loss: 1.0709, Accuracy: 0.5644\n",
            "==============================\n",
            "TRAIN: Epoch 487, Average Loss: 0.2712, Accuracy: 0.9052\n",
            "TEST: Epoch 487, Average Loss: 1.0687, Accuracy: 0.5619\n",
            "==============================\n",
            "TRAIN: Epoch 488, Average Loss: 0.2744, Accuracy: 0.8853\n",
            "TEST: Epoch 488, Average Loss: 1.0693, Accuracy: 0.5613\n",
            "==============================\n",
            "TRAIN: Epoch 489, Average Loss: 0.2763, Accuracy: 0.8928\n",
            "TEST: Epoch 489, Average Loss: 1.0702, Accuracy: 0.5426\n",
            "==============================\n",
            "TRAIN: Epoch 490, Average Loss: 0.2673, Accuracy: 0.9202\n",
            "TEST: Epoch 490, Average Loss: 1.0761, Accuracy: 0.5607\n",
            "==============================\n",
            "TRAIN: Epoch 491, Average Loss: 0.2511, Accuracy: 0.9052\n",
            "TEST: Epoch 491, Average Loss: 1.0797, Accuracy: 0.5607\n",
            "==============================\n",
            "TRAIN: Epoch 492, Average Loss: 0.2379, Accuracy: 0.9102\n",
            "TEST: Epoch 492, Average Loss: 1.0826, Accuracy: 0.5451\n",
            "==============================\n",
            "TRAIN: Epoch 493, Average Loss: 0.2519, Accuracy: 0.9202\n",
            "TEST: Epoch 493, Average Loss: 1.1061, Accuracy: 0.5526\n",
            "==============================\n",
            "TRAIN: Epoch 494, Average Loss: 0.2466, Accuracy: 0.9127\n",
            "TEST: Epoch 494, Average Loss: 1.0757, Accuracy: 0.5507\n",
            "==============================\n",
            "TRAIN: Epoch 495, Average Loss: 0.2672, Accuracy: 0.9027\n",
            "TEST: Epoch 495, Average Loss: 1.0976, Accuracy: 0.5632\n",
            "==============================\n",
            "TRAIN: Epoch 496, Average Loss: 0.2484, Accuracy: 0.9177\n",
            "TEST: Epoch 496, Average Loss: 1.0778, Accuracy: 0.5364\n",
            "==============================\n",
            "TRAIN: Epoch 497, Average Loss: 0.2582, Accuracy: 0.9002\n",
            "TEST: Epoch 497, Average Loss: 1.1059, Accuracy: 0.5769\n",
            "==============================\n",
            "TRAIN: Epoch 498, Average Loss: 0.2702, Accuracy: 0.8803\n",
            "TEST: Epoch 498, Average Loss: 1.0767, Accuracy: 0.5464\n",
            "==============================\n",
            "TRAIN: Epoch 499, Average Loss: 0.2354, Accuracy: 0.9177\n",
            "TEST: Epoch 499, Average Loss: 1.0934, Accuracy: 0.5488\n",
            "==============================\n",
            "TRAIN: Epoch 500, Average Loss: 0.2304, Accuracy: 0.9202\n",
            "TEST: Epoch 500, Average Loss: 1.0802, Accuracy: 0.5513\n",
            "==============================\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 500\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    model.train()  # Переключение в режим обучения\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted_classes = torch.max(outputs, 1)\n",
        "        correct_predictions = (predicted_classes == targets).float()\n",
        "        total_correct += correct_predictions.sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"TRAIN: Epoch {epoch + 1}, Average Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted_classes = torch.max(outputs, 1)\n",
        "            correct_predictions = (predicted_classes == targets).float()\n",
        "            total_correct += correct_predictions.sum().item()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "    average_loss = total_loss / len(test_loader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"TEST: Epoch {epoch + 1}, Average Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    print('='*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wiwfsum80PZx"
      },
      "source": [
        "## Test bert for embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Правильно обработать bert "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb-Nhuk5-chi"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEQ35CQd0PZx",
        "outputId": "c6f028da-7f26-44e0-aaee-30e4ba6dc1f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    1308\n",
              "1     701\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res['target'] = res['target'].apply(lambda x: target_labeler(x))\n",
        "res['target'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKmWpQOOG-EC",
        "outputId": "39aa4d71-296f-4b48-8437-0b2b30f7bc4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    712\n",
              "1    701\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res.drop(neg_filtering_bad_idx)['target'].value_counts()\n",
        "res.drop(pos_filtering_bad_idx)['target'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MU32I4-MB8sz",
        "outputId": "4a5bd963-196f-4564-ca56-c697879a9611"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"res\",\n  \"rows\": 2009,\n  \"fields\": [\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2009,\n        \"samples\": [\n          \"\\u0427\\u0430\\u0439 \\u0413\\u043b\\u0438\\u043d\\u0442\\u0432\\u0435\\u0439\\u043d - \\u0431\\u043e\\u0433\\u0430\\u0442\\u0430\\u044f \\u0440\\u0430\\u0437\\u043d\\u043e\\u043e\\u0431\\u0440\\u0430\\u0437\\u0438\\u0435\\u043c \\u0441\\u0443\\u0445\\u0430\\u044f \\u0441\\u043c\\u0435\\u0441\\u044c \\u0441\\u043e\\u0434\\u0435\\u0440\\u0436\\u0438\\u0442 \\u0432\\u0441\\u0435 \\u043e\\u0442\\u0442\\u0435\\u043d\\u043a\\u0438 \\u043e\\u0442 \\u0441\\u0432\\u0435\\u0442\\u043b\\u043e-\\u043f\\u0435\\u0441\\u043e\\u0447\\u043d\\u043e\\u0433\\u043e \\u0434\\u043e \\u0442\\u0435\\u043c\\u043d\\u043e-\\u0431\\u043e\\u0440\\u0434\\u043e\\u0432\\u043e\\u0433\\u043e, \\u0430\\u043a\\u0446\\u0435\\u043d\\u0442\\u0430\\u043c\\u0438 \\u0440\\u0430\\u0441\\u0441\\u044b\\u043f\\u0430\\u043d\\u044b \\u0437\\u0435\\u043b\\u0435\\u043d\\u044b\\u0435 \\u043a\\u043e\\u0440\\u043e\\u0431\\u043e\\u0447\\u043a\\u0438 \\u043a\\u0430\\u0440\\u0434\\u0430\\u043c\\u043e\\u043d\\u0430. \\u0412 \\u043a\\u0430\\u0436\\u0434\\u043e\\u043c \\u0433\\u043b\\u043e\\u0442\\u043a\\u0435 \\u0433\\u043e\\u0442\\u043e\\u0432\\u043e\\u0433\\u043e \\u043d\\u0430\\u043f\\u0438\\u0442\\u043a\\u0430 \\u043f\\u0435\\u0440\\u0435\\u043f\\u043b\\u0435\\u0442\\u0430\\u0435\\u0442\\u0441\\u044f \\u0444\\u0440\\u0443\\u043a\\u0442\\u043e\\u0432\\u0430\\u044f \\u043a\\u0438\\u0441\\u043b\\u0438\\u043d\\u043a\\u0430, \\u043b\\u0435\\u0433\\u043a\\u0430\\u044f \\u0441\\u043b\\u0430\\u0434\\u043e\\u0441\\u0442\\u044c \\u0438 \\u043f\\u0440\\u044f\\u043d\\u044b\\u0435 \\u043d\\u043e\\u0442\\u043a\\u0438. \\u0410\\u0440\\u043e\\u043c\\u0430\\u0442 \\u043f\\u0440\\u044f\\u043d\\u043e\\u0441\\u0442\\u0435\\u0439 \\u0441\\u043e\\u0437\\u0434\\u0430\\u0435\\u0442 \\u0430\\u0442\\u043c\\u043e\\u0441\\u0444\\u0435\\u0440\\u0443 \\u0442\\u0435\\u043f\\u043b\\u0430 \\u0438 \\u0443\\u044e\\u0442\\u0430. \",\n          \"\\u0418\\u043d\\u0434\\u0438\\u0439\\u0441\\u043a\\u0438\\u0439 \\u0447\\u0430\\u0439 \\u0410\\u0441\\u0441\\u0430\\u043c - \\u043a\\u043b\\u0430\\u0441\\u0441\\u0438\\u043a\\u0430 \\u0447\\u0435\\u0440\\u043d\\u043e\\u0433\\u043e \\u0447\\u0430\\u044f \\u0437\\u043d\\u0430\\u043a\\u043e\\u043c\\u0430\\u044f \\u0432\\u0441\\u0435\\u043c. \\u0417\\u0430\\u0432\\u0430\\u0440\\u0435\\u043d\\u043d\\u044b\\u0439 \\u0447\\u0430\\u0439 \\u043f\\u0440\\u0438\\u043e\\u0431\\u0440\\u0435\\u0442\\u0430\\u0435\\u0442 \\u0442\\u0435\\u043c\\u043d\\u043e-\\u044f\\u043d\\u0442\\u0430\\u0440\\u043d\\u044b\\u0439 \\u043e\\u0442\\u0442\\u0435\\u043d\\u043e\\u043a, \\u0441\\u0431\\u0430\\u043b\\u0430\\u043d\\u0441\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u043d\\u044b\\u0439 \\u0431\\u0430\\u0440\\u0445\\u0430\\u0442\\u0438\\u0441\\u0442\\u044b\\u0439 \\u0432\\u043a\\u0443\\u0441 \\u0441 \\u043b\\u0435\\u0433\\u043a\\u043e\\u0439 \\u0433\\u043e\\u0440\\u0447\\u0438\\u043d\\u043a\\u043e\\u0439, \\u0443\\u043c\\u0435\\u0440\\u0435\\u043d\\u043d\\u043e\\u0439 \\u0442\\u0435\\u0440\\u043f\\u043a\\u043e\\u0441\\u0442\\u044c\\u044e \\u0438 \\u0434\\u043b\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u043c \\u043c\\u0435\\u0434\\u043e\\u0432\\u044b\\u043c \\u043f\\u043e\\u0441\\u043b\\u0435\\u0432\\u043a\\u0443\\u0441\\u0438\\u0435\\u043c. \\u0420\\u0430\\u0437\\u043d\\u043e\\u043e\\u0431\\u0440\\u0430\\u0437\\u0438\\u0442\\u044c \\u043f\\u0440\\u0438\\u0432\\u044b\\u0447\\u043d\\u043e\\u0435 \\u0447\\u0430\\u0435\\u043f\\u0438\\u0442\\u0438\\u0435 \\u043f\\u043e\\u043c\\u043e\\u0433\\u0443\\u0442 \\u0444\\u0440\\u0443\\u043a\\u0442\\u043e\\u0432\\u044b\\u0435 \\u043d\\u043e\\u0442\\u044b \\u044d\\u043a\\u0437\\u043e\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0433\\u043e \\u043c\\u0430\\u043d\\u0433\\u043e \\u0438 \\u043c\\u0430\\u0440\\u0430\\u043a\\u0443\\u0439\\u0438.\\r\\n\\r\\n\\u0427\\u0430\\u0439 \\u0410\\u0441\\u0441\\u0430\\u043c \\u043f\\u0440\\u0435\\u043a\\u0440\\u0430\\u0441\\u043d\\u043e \\u043f\\u043e\\u0434\\u043e\\u0439\\u0434\\u0435\\u0442 \\u0434\\u043b\\u044f \\u0443\\u0442\\u0440\\u0435\\u043d\\u043d\\u0435\\u0433\\u043e \\u0438\\u043b\\u0438 \\u0432\\u0435\\u0447\\u0435\\u0440\\u043d\\u0435\\u0433\\u043e \\u0447\\u0430\\u0435\\u043f\\u0438\\u0442\\u0438\\u044f. \\u0410\\u0440\\u043e\\u043c\\u0430\\u0442\\u043d\\u044b\\u0439 \\u043d\\u0430\\u043f\\u0438\\u0442\\u043e\\u043a \\u0441\\u0442\\u0430\\u043d\\u0435\\u0442 \\u043e\\u0442\\u043b\\u0438\\u0447\\u043d\\u044b\\u043c \\u0434\\u043e\\u043f\\u043e\\u043b\\u043d\\u0435\\u043d\\u0438\\u0435\\u043c \\u043a \\u043f\\u0440\\u0430\\u0437\\u0434\\u043d\\u0438\\u0447\\u043d\\u043e\\u043c\\u0443 \\u0441\\u0442\\u043e\\u043b\\u0443 \\u0438 \\u043f\\u043e\\u043c\\u043e\\u0436\\u0435\\u0442 \\u0443\\u0434\\u0438\\u0432\\u0438\\u0442\\u044c \\u0431\\u043b\\u0438\\u0437\\u043a\\u0438\\u0445.\\r\\n\\r\\n\\u041a\\u0430\\u043a \\u0437\\u0430\\u0432\\u0430\\u0440\\u0438\\u0432\\u0430\\u0442\\u044c \\u0447\\u0435\\u0440\\u043d\\u044b\\u0439 \\u0447\\u0430\\u0439 \\u0410\\u0441\\u0441\\u0430\\u043c?\\r\\n\\r\\n1 \\u0447\\u0430\\u0439\\u043d\\u0443\\u044e \\u043b\\u043e\\u0436\\u043a\\u0443 \\u0447\\u0430\\u044f \\u0437\\u0430\\u043b\\u0438\\u0442\\u044c 250 \\u043c\\u043b \\u0433\\u043e\\u0440\\u044f\\u0447\\u0435\\u0439 \\u0432\\u043e\\u0434\\u044b \\u0442\\u0435\\u043c\\u043f\\u0435\\u0440\\u0430\\u0442\\u0443\\u0440\\u043e\\u0439 90-95\\u00b0\\u0421, \\u0434\\u0430\\u0442\\u044c \\u043d\\u0430\\u0441\\u0442\\u043e\\u044f\\u0442\\u044c\\u0441\\u044f 2-3 \\u043c\\u0438\\u043d\\u0443\\u0442\\u044b.\\r\\n\",\n          \"\\u0414\\u043b\\u044f \\u043e\\u0441\\u043e\\u0431\\u0435\\u043d\\u043d\\u043e \\u0442\\u0440\\u0435\\u0431\\u043e\\u0432\\u0430\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0445 \\u0436\\u0435\\u043d\\u0449\\u0438\\u043d, \\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0435 \\u0438\\u0449\\u0443\\u0442 \\u043f\\u0443\\u0442\\u044c \\u043a \\u0443\\u0441\\u043f\\u043e\\u043a\\u043e\\u0435\\u043d\\u0438\\u044e, \\u043d\\u0430\\u0442\\u0443\\u0440\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0447\\u0430\\u0439 \\u043e\\u0442 \\u0441\\u0442\\u0440\\u0435\\u0441\\u0441\\u0430 - \\u043d\\u0430\\u0445\\u043e\\u0434\\u043a\\u0430. \\u041e\\u043d \\u0441\\u043e\\u0434\\u0435\\u0440\\u0436\\u0438\\u0442 \\u0442\\u0440\\u0430\\u0432\\u044b, \\u0441\\u043f\\u0435\\u0446\\u0438\\u0430\\u043b\\u044c\\u043d\\u043e \\u043e\\u0442\\u043e\\u0431\\u0440\\u0430\\u043d\\u043d\\u044b\\u0435 \\u0434\\u043b\\u044f \\u043e\\u0431\\u0435\\u0441\\u043f\\u0435\\u0447\\u0435\\u043d\\u0438\\u044f \\u0434\\u043e\\u043b\\u0433\\u043e\\u0436\\u0434\\u0430\\u043d\\u043d\\u043e\\u0433\\u043e \\u0440\\u0435\\u043b\\u0430\\u043a\\u0441\\u0430 \\u0438 \\u0443\\u043b\\u0443\\u0447\\u0448\\u0435\\u043d\\u0438\\u044f \\u0441\\u0430\\u043c\\u043e\\u0447\\u0443\\u0432\\u0441\\u0442\\u0432\\u0438\\u044f. \\u041f\\u0443\\u0441\\u0442\\u044b\\u0440\\u043d\\u0438\\u043a \\u0438 \\u0434\\u0440\\u0443\\u0433\\u0438\\u0435 \\u0443\\u0441\\u043f\\u043e\\u043a\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0435 \\u0442\\u0440\\u0430\\u0432\\u044b \\u043f\\u043e\\u043c\\u043e\\u0433\\u0443\\u0442 \\u0431\\u044b\\u0441\\u0442\\u0440\\u043e \\u0441\\u043d\\u044f\\u0442\\u044c \\u0441\\u0442\\u0440\\u0435\\u0441\\u0441 \\u0438 \\u0443\\u0441\\u0442\\u0430\\u043b\\u043e\\u0441\\u0442\\u044c, \\u0441\\u043e\\u0437\\u0434\\u0430\\u0432 \\u043e\\u0449\\u0443\\u0449\\u0435\\u043d\\u0438\\u0435 \\u0443\\u043c\\u0438\\u0440\\u043e\\u0442\\u0432\\u043e\\u0440\\u0435\\u043d\\u0438\\u044f \\u0438 \\u0441\\u043f\\u043e\\u043a\\u043e\\u0439\\u0441\\u0442\\u0432\\u0438\\u044f.\\r\\n\\r\\n\\u0422\\u0440\\u0430\\u0432\\u044f\\u043d\\u044b\\u0435 \\u0447\\u0430\\u0438 - \\u044d\\u0442\\u043e \\u043d\\u0435 \\u0442\\u043e\\u043b\\u044c\\u043a\\u043e \\u0432\\u043a\\u0443\\u0441\\u043d\\u044b\\u0439 \\u043d\\u0435\\u0430\\u043b\\u043a\\u043e\\u0433\\u043e\\u043b\\u044c\\u043d\\u044b\\u0439 \\u043d\\u0430\\u043f\\u0438\\u0442\\u043e\\u043a, \\u043d\\u043e \\u0438 \\u043d\\u0430\\u0441\\u0442\\u043e\\u044f\\u0449\\u0438\\u0439 \\u043f\\u043e\\u043c\\u043e\\u0449\\u043d\\u0438\\u043a \\u0434\\u043b\\u044f \\u0432\\u0441\\u0435\\u0445, \\u043a\\u0442\\u043e \\u0445\\u043e\\u0447\\u0435\\u0442 \\u0440\\u0430\\u0441\\u0441\\u043b\\u0430\\u0431\\u0438\\u0442\\u044c\\u0441\\u044f \\u0438 \\u043e\\u0442\\u0434\\u043e\\u0445\\u043d\\u0443\\u0442\\u044c. \\u041f\\u0440\\u0438\\u0433\\u043e\\u0442\\u043e\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0447\\u0430\\u044f \\u0434\\u043b\\u044f \\u0437\\u0430\\u0432\\u0430\\u0440\\u0438\\u0432\\u0430\\u043d\\u0438\\u044f - \\u044d\\u0442\\u043e \\u043d\\u0430\\u0441\\u0442\\u043e\\u044f\\u0449\\u0435\\u0435 \\u0438\\u0441\\u043a\\u0443\\u0441\\u0441\\u0442\\u0432\\u043e, \\u043a\\u043e\\u0442\\u043e\\u0440\\u043e\\u0435 \\u0434\\u0430\\u0440\\u0438\\u0442 \\u043d\\u0430\\u043c \\u043c\\u043e\\u043c\\u0435\\u043d\\u0442\\u044b \\u0443\\u043c\\u0438\\u0440\\u043e\\u0442\\u0432\\u043e\\u0440\\u0435\\u043d\\u0438\\u044f \\u0438 \\u043d\\u0430\\u0441\\u043b\\u0430\\u0436\\u0434\\u0435\\u043d\\u0438\\u044f. \\u0414\\u043e\\u0431\\u0430\\u0432\\u044c\\u0442\\u0435 \\u0432 \\u044d\\u0442\\u043e \\u0440\\u0438\\u0442\\u0443\\u0430\\u043b \\u043d\\u0435\\u043c\\u043d\\u043e\\u0433\\u043e \\u0443\\u0441\\u043f\\u043e\\u043a\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0445 \\u0442\\u0440\\u0430\\u0432, \\u0442\\u0430\\u043a\\u0438\\u0445 \\u043a\\u0430\\u043a \\u043f\\u0443\\u0441\\u0442\\u044b\\u0440\\u043d\\u0438\\u043a, \\u0432\\u0430\\u043b\\u0435\\u0440\\u0438\\u0430\\u043d\\u0430, \\u0438 \\u0432\\u044b \\u043f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u0435 \\u0438\\u0434\\u0435\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0447\\u0430\\u0439 \\u0434\\u043b\\u044f \\u0440\\u0430\\u0441\\u0441\\u043b\\u0430\\u0431\\u043b\\u0435\\u043d\\u0438\\u044f \\u0438 \\u0441\\u043d\\u0430.\\r\\n\\r\\n\\u0427\\u0430\\u0439 \\u0443\\u0441\\u043f\\u043e\\u043a\\u0430\\u0438\\u0432\\u0430\\u044e\\u0449\\u0438\\u0439 \\u0432 \\u043f\\u0430\\u043a\\u0435\\u0442\\u0438\\u043a\\u0430\\u0445 - \\u044d\\u0442\\u043e \\u043e\\u0442\\u043b\\u0438\\u0447\\u043d\\u044b\\u0439 \\u0432\\u0430\\u0440\\u0438\\u0430\\u043d\\u0442 \\u0434\\u043b\\u044f \\u0442\\u0435\\u0445, \\u043a\\u0442\\u043e \\u0446\\u0435\\u043d\\u0438\\u0442 \\u0443\\u0434\\u043e\\u0431\\u0441\\u0442\\u0432\\u043e \\u0438 \\u0431\\u044b\\u0441\\u0442\\u0440\\u043e\\u0442\\u0443 \\u043f\\u0440\\u0438\\u0433\\u043e\\u0442\\u043e\\u0432\\u043b\\u0435\\u043d\\u0438\\u044f. \\u0412\\u044b \\u043f\\u0440\\u043e\\u0441\\u0442\\u043e \\u043a\\u043b\\u0430\\u0434\\u0435\\u0442\\u0435 \\u043f\\u0430\\u043a\\u0435\\u0442\\u0438\\u043a \\u0432 \\u0447\\u0430\\u0448\\u043a\\u0443, \\u0437\\u0430\\u043b\\u0438\\u0432\\u0430\\u0435\\u0442\\u0435 \\u0433\\u043e\\u0440\\u044f\\u0447\\u0435\\u0439 \\u0432\\u043e\\u0434\\u043e\\u0439 \\u0438 \\u0447\\u0435\\u0440\\u0435\\u0437 \\u043f\\u0430\\u0440\\u0443 \\u043c\\u0438\\u043d\\u0443\\u0442 \\u043d\\u0430\\u0441\\u043b\\u0430\\u0436\\u0434\\u0430\\u0435\\u0442\\u0435\\u0441\\u044c \\u0432\\u043a\\u0443\\u0441\\u043e\\u043c \\u0438 \\u0430\\u0440\\u043e\\u043c\\u0430\\u0442\\u043e\\u043c \\u0437\\u0430\\u043c\\u0435\\u0447\\u0430\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0433\\u043e \\u0447\\u0430\\u044f. \\u0410 \\u0443\\u0441\\u043f\\u043e\\u043a\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0439 \\u044d\\u0444\\u0444\\u0435\\u043a\\u0442 \\u043d\\u0435 \\u0437\\u0430\\u0441\\u0442\\u0430\\u0432\\u0438\\u0442 \\u0432\\u0430\\u0441 \\u0436\\u0434\\u0430\\u0442\\u044c - \\u0443\\u0436\\u0435 \\u043f\\u043e\\u0441\\u043b\\u0435 \\u043f\\u0435\\u0440\\u0432\\u043e\\u0433\\u043e \\u0433\\u043b\\u043e\\u0442\\u043a\\u0430 \\u0432\\u044b \\u043f\\u043e\\u0447\\u0443\\u0432\\u0441\\u0442\\u0432\\u0443\\u0435\\u0442\\u0435, \\u043a\\u0430\\u043a \\u043d\\u0430\\u043f\\u0440\\u044f\\u0436\\u0435\\u043d\\u0438\\u0435 \\u043f\\u043e\\u043a\\u0438\\u0434\\u0430\\u0435\\u0442 \\u0432\\u0430\\u0448\\u0435 \\u0442\\u0435\\u043b\\u043e, \\u0430 \\u0443\\u043c\\u0438\\u0440\\u043e\\u0442\\u0432\\u043e\\u0440\\u0435\\u043d\\u0438\\u0435 \\u043d\\u0430\\u043f\\u043e\\u043b\\u043d\\u044f\\u0435\\u0442 \\u0434\\u0443\\u0448\\u0443.\\r\\n\\r\\n\\u0410\\u043b\\u0442\\u0430\\u0439\\u0441\\u043a\\u0438\\u0435 \\u0442\\u0440\\u0430\\u0432\\u044b \\u044f\\u0432\\u043b\\u044f\\u044e\\u0442\\u0441\\u044f \\u043d\\u0430\\u0441\\u0442\\u043e\\u044f\\u0449\\u0438\\u043c \\u0441\\u043e\\u043a\\u0440\\u043e\\u0432\\u0438\\u0449\\u0435\\u043c \\u043f\\u0440\\u0438\\u0440\\u043e\\u0434\\u044b. \\u0418\\u0445 \\u043b\\u0435\\u0447\\u0435\\u0431\\u043d\\u044b\\u0435 \\u0441\\u0432\\u043e\\u0439\\u0441\\u0442\\u0432\\u0430 \\u0438\\u0437\\u0432\\u0435\\u0441\\u0442\\u043d\\u044b \\u0443\\u0436\\u0435 \\u043c\\u043d\\u043e\\u0433\\u0438\\u0435 \\u0432\\u0435\\u043a\\u0430. \\u0427\\u0430\\u0439 \\u0438\\u0437 \\u0430\\u043b\\u0442\\u0430\\u0439\\u0441\\u043a\\u0438\\u0445 \\u0442\\u0440\\u0430\\u0432 - \\u044d\\u0442\\u043e \\u043d\\u0430\\u0441\\u0442\\u043e\\u044f\\u0449\\u0438\\u0439 \\u043b\\u0435\\u043a\\u0430\\u0440\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u044b\\u0439 \\u0437\\u0435\\u043b\\u044c\\u0435, \\u043a\\u043e\\u0442\\u043e\\u0440\\u043e\\u0435 \\u043f\\u043e\\u043c\\u043e\\u0433\\u0430\\u0435\\u0442 \\u0441\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u044c\\u0441\\u044f \\u0441\\u043e \\u043c\\u043d\\u043e\\u0433\\u0438\\u043c\\u0438 \\u043f\\u0440\\u043e\\u0431\\u043b\\u0435\\u043c\\u0430\\u043c\\u0438. \\u0412 \\u043d\\u0435\\u043c \\u0441\\u043e\\u0431\\u0440\\u0430\\u043d\\u044b \\u0434\\u0430\\u0440\\u044b \\u043f\\u0440\\u0438\\u0440\\u043e\\u0434\\u044b, \\u0442\\u0430\\u043a\\u0438\\u0435 \\u043a\\u0430\\u043a \\u043f\\u0443\\u0441\\u0442\\u044b\\u0440\\u043d\\u0438\\u043a, \\u0432\\u0430\\u043b\\u0435\\u0440\\u0438\\u0430\\u043d\\u0430, \\u0447\\u0430\\u0433\\u0430 \\u0438 \\u0434\\u0440\\u0443\\u0433\\u0438\\u0435 \\u0442\\u0440\\u0430\\u0432\\u044b, \\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0435 \\u043e\\u0434\\u043d\\u043e\\u0432\\u0440\\u0435\\u043c\\u0435\\u043d\\u043d\\u043e \\u0443\\u0441\\u043f\\u043e\\u043a\\u0430\\u0438\\u0432\\u0430\\u044e\\u0442 \\u0438 \\u0443\\u043a\\u0440\\u0435\\u043f\\u043b\\u044f\\u044e\\u0442 \\u043d\\u0430\\u0448 \\u043e\\u0440\\u0433\\u0430\\u043d\\u0438\\u0437\\u043c.\\r\\n\\r\\n\\u041c\\u043e\\u043d\\u0430\\u0441\\u0442\\u044b\\u0440\\u0441\\u043a\\u0438\\u0439 \\u0441\\u0431\\u043e\\u0440 \\u0442\\u0440\\u0430\\u0432 - \\u044d\\u0442\\u043e \\u0447\\u0443\\u0434\\u0435\\u0441\\u043d\\u043e\\u0435 \\u0441\\u043e\\u0447\\u0435\\u0442\\u0430\\u043d\\u0438\\u0435 \\u0440\\u0430\\u0437\\u043b\\u0438\\u0447\\u043d\\u044b\\u0445 \\u0440\\u0430\\u0441\\u0442\\u0435\\u043d\\u0438\\u0439, \\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0435 \\u0431\\u043b\\u0430\\u0433\\u043e\\u0442\\u0432\\u043e\\u0440\\u043d\\u043e \\u0432\\u043b\\u0438\\u044f\\u044e\\u0442 \\u043d\\u0430 \\u043d\\u0430\\u0448\\u0435 \\u0444\\u0438\\u0437\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0435 \\u0438 \\u043f\\u0441\\u0438\\u0445\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0435 \\u0441\\u043e\\u0441\\u0442\\u043e\\u044f\\u043d\\u0438\\u0435. \\u0412 \\u043d\\u0435\\u043c \\u0441\\u043e\\u0431\\u0440\\u0430\\u043d\\u044b \\u0441\\u0430\\u043c\\u044b\\u0435 \\u043b\\u0443\\u0447\\u0448\\u0438\\u0435 \\u043b\\u0435\\u043a\\u0430\\u0440\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u044b\\u0435 \\u0442\\u0440\\u0430\\u0432\\u044b, \\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0435 \\u043f\\u043e\\u043c\\u043e\\u0433\\u0430\\u044e\\u0442 \\u0443\\u043a\\u0440\\u0435\\u043f\\u0438\\u0442\\u044c \\u0438\\u043c\\u043c\\u0443\\u043d\\u0438\\u0442\\u0435\\u0442, \\u043f\\u043e\\u0432\\u044b\\u0441\\u0438\\u0442\\u044c \\u044d\\u043d\\u0435\\u0440\\u0433\\u0438\\u044e \\u0438 \\u043e\\u0431\\u0435\\u0441\\u043f\\u0435\\u0447\\u0438\\u0442\\u044c \\u0433\\u0430\\u0440\\u043c\\u043e\\u043d\\u0438\\u044e \\u0432 \\u043e\\u0440\\u0433\\u0430\\u043d\\u0438\\u0437\\u043c\\u0435. \\u041f\\u043e\\u043f\\u0440\\u043e\\u0431\\u0443\\u0439\\u0442\\u0435 \\u043c\\u043e\\u043d\\u0430\\u0441\\u0442\\u044b\\u0440\\u0441\\u043a\\u0438\\u0439 \\u0441\\u0431\\u043e\\u0440 \\u0442\\u0440\\u0430\\u0432 \\u0438 \\u0432\\u044b \\u043f\\u043e\\u0447\\u0443\\u0432\\u0441\\u0442\\u0432\\u0443\\u0435\\u0442\\u0435, \\u043a\\u0430\\u043a \\u0432\\u0430\\u0448\\u0435 \\u0442\\u0435\\u043b\\u043e \\u043e\\u0437\\u0434\\u043e\\u0440\\u0430\\u0432\\u043b\\u0438\\u0432\\u0430\\u0435\\u0442\\u0441\\u044f \\u0438 \\u043d\\u0430\\u043f\\u043e\\u043b\\u043d\\u044f\\u0435\\u0442\\u0441\\u044f \\u0441\\u0438\\u043b\\u043e\\u0439.\\r\\n\\r\\n\\u0423\\u0441\\u043f\\u043e\\u043a\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0447\\u0430\\u0439 - \\u044d\\u0442\\u043e \\u043d\\u0430\\u0441\\u0442\\u043e\\u044f\\u0449\\u0430\\u044f \\u043d\\u0430\\u0445\\u043e\\u0434\\u043a\\u0430 \\u0434\\u043b\\u044f \\u0432\\u0437\\u0440\\u043e\\u0441\\u043b\\u044b\\u0445, \\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0435 \\u0445\\u043e\\u0442\\u044f\\u0442 \\u0441\\u043d\\u044f\\u0442\\u044c \\u043d\\u0430\\u043f\\u0440\\u044f\\u0436\\u0435\\u043d\\u0438\\u0435 \\u0438 \\u0443\\u0441\\u043f\\u043e\\u043a\\u043e\\u0438\\u0442\\u044c\\u0441\\u044f \\u043f\\u043e\\u0441\\u043b\\u0435 \\u0442\\u0440\\u0443\\u0434\\u043d\\u043e\\u0433\\u043e \\u0434\\u043d\\u044f. \\u0412 \\u043d\\u0435\\u043c \\u0441\\u043e\\u0431\\u0440\\u0430\\u043d\\u044b \\u0441\\u043f\\u0435\\u0446\\u0438\\u0430\\u043b\\u044c\\u043d\\u043e \\u043e\\u0442\\u043e\\u0431\\u0440\\u0430\\u043d\\u043d\\u044b\\u0435 \\u0442\\u0440\\u0430\\u0432\\u044b, \\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0435 \\u043f\\u043e\\u043c\\u043e\\u0433\\u0443\\u0442 \\u0441\\u043d\\u044f\\u0442\\u044c \\u0441\\u0442\\u0440\\u0435\\u0441\\u0441, \\u0443\\u043b\\u0443\\u0447\\u0448\\u0438\\u0442\\u044c \\u043a\\u0430\\u0447\\u0435\\u0441\\u0442\\u0432\\u043e \\u0441\\u043d\\u0430 \\u0438 \\u0441\\u043e\\u0437\\u0434\\u0430\\u0442\\u044c \\u043e\\u0449\\u0443\\u0449\\u0435\\u043d\\u0438\\u0435 \\u043f\\u043e\\u043a\\u043e\\u044f \\u0438 \\u0441\\u043f\\u043e\\u043a\\u043e\\u0439\\u0441\\u0442\\u0432\\u0438\\u044f. \\u041f\\u0443\\u0441\\u0442\\u044b\\u0440\\u043d\\u0438\\u043a, \\u0432\\u0430\\u043b\\u0435\\u0440\\u0438\\u0430\\u043d\\u0430 \\u0438 \\u0434\\u0440\\u0443\\u0433\\u0438\\u0435 \\u0440\\u0430\\u0441\\u0442\\u0435\\u043d\\u0438\\u044f \\u043f\\u043e\\u043c\\u043e\\u0433\\u0443\\u0442 \\u0432\\u0430\\u043c \\u043d\\u0430\\u0441\\u043b\\u0430\\u0434\\u0438\\u0442\\u044c\\u0441\\u044f \\u043c\\u043e\\u043c\\u0435\\u043d\\u0442\\u0430\\u043c\\u0438 \\u0443\\u043c\\u0438\\u0440\\u043e\\u0442\\u0432\\u043e\\u0440\\u0435\\u043d\\u0438\\u044f \\u0438 \\u0440\\u0430\\u0441\\u0441\\u043b\\u0430\\u0431\\u043b\\u0435\\u043d\\u0438\\u044f.\\r\\n\\r\\n\\r\\n\\u041d\\u0435 \\u0437\\u0430\\u0431\\u044b\\u0432\\u0430\\u0439\\u0442\\u0435 \\u043e \\u0442\\u043e\\u043c, \\u0447\\u0442\\u043e \\u0440\\u0435\\u0433\\u0443\\u043b\\u044f\\u0440\\u043d\\u043e\\u0435 \\u0443\\u043f\\u043e\\u0442\\u0440\\u0435\\u0431\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0443\\u0441\\u043f\\u043e\\u043a\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0433\\u043e \\u0447\\u0430\\u044f \\u043c\\u043e\\u0436\\u043d\\u043e \\u0438\\u0441\\u043f\\u043e\\u043b\\u044c\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0434\\u043b\\u044f \\u043f\\u0440\\u043e\\u0444\\u0438\\u043b\\u0430\\u043a\\u0442\\u0438\\u043a\\u0438 \\u0438 \\u0443\\u043b\\u0443\\u0447\\u0448\\u0435\\u043d\\u0438\\u044f \\u043e\\u0431\\u0449\\u0435\\u0433\\u043e \\u0441\\u043e\\u0441\\u0442\\u043e\\u044f\\u043d\\u0438\\u044f \\u0437\\u0434\\u043e\\u0440\\u043e\\u0432\\u044c\\u044f. \\u041e\\u043d \\u043d\\u0435 \\u0442\\u043e\\u043b\\u044c\\u043a\\u043e \\u043f\\u043e\\u043c\\u043e\\u0433\\u0430\\u0435\\u0442 \\u0441\\u043d\\u044f\\u0442\\u044c \\u043d\\u0430\\u043f\\u0440\\u044f\\u0436\\u0435\\u043d\\u0438\\u0435 \\u0438 \\u0443\\u043b\\u0443\\u0447\\u0448\\u0438\\u0442\\u044c \\u043a\\u0430\\u0447\\u0435\\u0441\\u0442\\u0432\\u043e \\u0441\\u043d\\u0430, \\u043d\\u043e \\u0438 \\u0438\\u043c\\u0435\\u0435\\u0442 \\u043f\\u043e\\u043b\\u043e\\u0436\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0435 \\u0432\\u043b\\u0438\\u044f\\u043d\\u0438\\u0435 \\u043d\\u0430 \\u043d\\u0430\\u0448\\u0435 \\u0444\\u0438\\u0437\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0435 \\u0438 \\u044d\\u043c\\u043e\\u0446\\u0438\\u043e\\u043d\\u0430\\u043b\\u044c\\u043d\\u043e\\u0435 \\u0431\\u043b\\u0430\\u0433\\u043e\\u043f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0435. \\u041f\\u043e\\u043f\\u0440\\u043e\\u0431\\u0443\\u0439\\u0442\\u0435 \\u0442\\u0440\\u0430\\u0432\\u044f\\u043d\\u043e\\u0439 \\u0447\\u0430\\u0439 \\u0443\\u0441\\u043f\\u043e\\u043a\\u0430\\u0438\\u0432\\u0430\\u044e\\u0449\\u0438\\u0439 \\u0438 \\u0432\\u044b \\u043f\\u043e\\u0447\\u0443\\u0432\\u0441\\u0442\\u0432\\u0443\\u0435\\u0442\\u0435, \\u043a\\u0430\\u043a \\u0432\\u0430\\u0448\\u0430 \\u0436\\u0438\\u0437\\u043d\\u044c \\u043d\\u0430\\u043f\\u043e\\u043b\\u043d\\u044f\\u0435\\u0442\\u0441\\u044f \\u0443\\u043c\\u0438\\u0440\\u043e\\u0442\\u0432\\u043e\\u0440\\u0435\\u043d\\u0438\\u0435\\u043c \\u0438 \\u0433\\u0430\\u0440\\u043c\\u043e\\u043d\\u0438\\u0435\\u0439. \\r\\n\\u0422\\u0440\\u0430\\u0432\\u044f\\u043d\\u043e\\u0439 \\u0441\\u0431\\u043e\\u0440  \\u043d\\u0435 \\u0441\\u043e\\u0434\\u0435\\u0440\\u0436\\u0438\\u0442 \\u043a\\u043e\\u0444\\u0435\\u0438\\u043d \\u0438 \\u043f\\u043e\\u044d\\u0442\\u043e\\u043b\\u044c\\u043a\\u043e 100% \\u041d\\u0430\\u0442\\u0443\\u0440\\u0430\\u043b\\u044c\\u043d\\u043e\\u0435 \\u0441\\u044b\\u0440\\u044c\\u0435. \\u041d\\u0438\\u043a\\u0430\\u043a\\u0438\\u0445 \\u0443\\u0441\\u0438\\u043b\\u0438\\u0442\\u0435\\u043b\\u0435\\u0439 \\u0432\\u043a\\u0443\\u0441\\u0430 \\u0438 \\u0430\\u0440\\u043e\\u043c\\u0430\\u0442\\u0430, \\u043a\\u0440\\u0430\\u0441\\u0438\\u0442\\u0435\\u043b\\u0435\\u0439. \\u0412 \\u0431\\u043e\\u043b\\u044c\\u0448\\u0438\\u043d\\u0441\\u0442\\u0432\\u0435 \\u043d\\u0430\\u0448\\u0438\\u0445 \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u043e\\u0432 \\u0441\\u043e\\u0434\\u0435\\u0440\\u0436\\u0438\\u0442\\u0441\\u044f \\u0411\\u0435\\u0440\\u0435\\u0437\\u043e\\u0432\\u0430\\u044f \\u0427\\u0430\\u0433\\u0430, \\u043e \\u043f\\u043e\\u043b\\u044c\\u0437\\u0435 \\u043a\\u043e\\u0442\\u043e\\u0440\\u043e\\u0439 \\u0445\\u043e\\u0434\\u044f\\u0442 \\u043b\\u0435\\u0433\\u0435\\u043d\\u0434\\u044b.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "res"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b714c91b-f045-4446-83b2-c8130e971b35\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\r\\nВРЕМЯ ПИТЬ ЧАЙ!\\r\\nBrusnikaTea «ЯГОДНЫЙ» -...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\r\\nВкусный чай зеленый листовой ягодами земля...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\r\\nИван чай Глазова Гора – традиционный русск...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\r\\nИщете идеальный подарок на Новый год? Пода...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\r\\nНастоящий Молочный улун высший сорт из Кит...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004</th>\n",
              "      <td>Ярко выраженное восточное благоухание зелёного...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005</th>\n",
              "      <td>пакетиках 14 матча день зеленый черный рождени...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006</th>\n",
              "      <td>улун дыня  - разновидность крупнолистового чая...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007</th>\n",
              "      <td>чай чёрный, рассыпной, масала, специи, саган д...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008</th>\n",
              "      <td>Описания нет</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2009 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b714c91b-f045-4446-83b2-c8130e971b35')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b714c91b-f045-4446-83b2-c8130e971b35 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b714c91b-f045-4446-83b2-c8130e971b35');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-204acecc-9946-42ad-9407-5da538fd10be\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-204acecc-9946-42ad-9407-5da538fd10be')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-204acecc-9946-42ad-9407-5da538fd10be button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                comment  target\n",
              "0     \\r\\nВРЕМЯ ПИТЬ ЧАЙ!\\r\\nBrusnikaTea «ЯГОДНЫЙ» -...       1\n",
              "1     \\r\\nВкусный чай зеленый листовой ягодами земля...       0\n",
              "2     \\r\\nИван чай Глазова Гора – традиционный русск...       0\n",
              "3     \\r\\nИщете идеальный подарок на Новый год? Пода...       0\n",
              "4     \\r\\nНастоящий Молочный улун высший сорт из Кит...       1\n",
              "...                                                 ...     ...\n",
              "2004  Ярко выраженное восточное благоухание зелёного...       0\n",
              "2005  пакетиках 14 матча день зеленый черный рождени...       0\n",
              "2006  улун дыня  - разновидность крупнолистового чая...       0\n",
              "2007  чай чёрный, рассыпной, масала, специи, саган д...       0\n",
              "2008                                       Описания нет       0\n",
              "\n",
              "[2009 rows x 2 columns]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbs832RV0PZ1"
      },
      "outputs": [],
      "source": [
        "def preprocess_for_bert(df, batch=32):\n",
        "\n",
        "  comments = df['comment'].to_list()\n",
        "  labels = df['target'].to_list()\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "  # Токенизация данных\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for comment in comments:\n",
        "      encoded_data = tokenizer.encode_plus(\n",
        "          comment,\n",
        "          add_special_tokens=True,\n",
        "          max_length=64,\n",
        "          pad_to_max_length=True,\n",
        "          return_attention_mask=True,\n",
        "          return_tensors='pt'\n",
        "      )\n",
        "      input_ids.append(encoded_data['input_ids'])\n",
        "      attention_masks.append(encoded_data['attention_mask'])\n",
        "\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.2)\n",
        "  train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.2)\n",
        "\n",
        "  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch)\n",
        "\n",
        "  validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch)\n",
        "\n",
        "  return train_data, train_sampler, train_dataloader, validation_data, validation_sampler, validation_dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ7KhX8_H7Hz"
      },
      "source": [
        "BERT on pos filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXMay-F2GwZs",
        "outputId": "1bebe7be-4975-4275-9a9b-0d5985d48d74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ],
      "source": [
        "train_data, train_sampler, train_dataloader, validation_data, validation_sampler, validation_dataloader = preprocess_for_bert(res.drop(pos_filtering_bad_idx), 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT46ikcS0PZ2"
      },
      "outputs": [],
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl6PY-AsIQh6",
        "outputId": "353a8de2-5a5b-46ac-b629-0e5fb02c6a7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cy6ns1s0PZ3",
        "outputId": "9138e807-eaa1-4314-8338-3b475aea5af5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 51/51 [00:17<00:00,  2.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.64\n",
            " Average training loss: 0.65\n",
            " Validation Accuracy: 0.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 51/51 [00:18<00:00,  2.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.66\n",
            " Average training loss: 0.64\n",
            " Validation Accuracy: 0.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 51/51 [00:18<00:00,  2.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.65\n",
            " Average training loss: 0.64\n",
            " Validation Accuracy: 0.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 51/51 [00:18<00:00,  2.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.66\n",
            " Average training loss: 0.63\n",
            " Validation Accuracy: 0.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 51/51 [00:18<00:00,  2.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.74\n",
            " Average training loss: 0.56\n",
            " Validation Accuracy: 0.56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 51/51 [00:18<00:00,  2.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.82\n",
            " Average training loss: 0.43\n",
            " Validation Accuracy: 0.59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 51/51 [00:18<00:00,  2.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.89\n",
            " Average training loss: 0.31\n",
            " Validation Accuracy: 0.55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 51/51 [00:18<00:00,  2.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.94\n",
            " Average training loss: 0.21\n",
            " Validation Accuracy: 0.56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 51/51 [00:18<00:00,  2.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.95\n",
            " Average training loss: 0.18\n",
            " Validation Accuracy: 0.59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 51/51 [00:18<00:00,  2.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.95\n",
            " Average training loss: 0.17\n",
            " Validation Accuracy: 0.62\n"
          ]
        }
      ],
      "source": [
        "train_loss_set = []\n",
        "train_accuracy_set = []\n",
        "validation_accuracy_set = []\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
        "model.dropout = nn.Dropout(0.2, inplace=False)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(768, 384),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(384,2),\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    total_train_accuracy = 0\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Epoch {}\".format(epoch))):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        logits = outputs.logits\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to(device).cpu().numpy()\n",
        "\n",
        "        total_train_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    train_accuracy_set.append(avg_train_accuracy)\n",
        "    train_loss_set.append(avg_train_loss)\n",
        "\n",
        "    print(\" Average training accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "    print(\" Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    validation_accuracy_set.append(avg_val_accuracy)\n",
        "    print(\" Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvdt82BJLdcY",
        "outputId": "70613c9d-6c7a-41a0-b5c1-306d6d657312"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ],
      "source": [
        "train_data, train_sampler, train_dataloader, validation_data, validation_sampler, validation_dataloader = preprocess_for_bert(res.drop(neg_filtering_bad_idx), 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt8Nsob8LmxJ",
        "outputId": "95e072d5-5382-4e1a-d0cf-8f9821c4309e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 51/51 [00:18<00:00,  2.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.65\n",
            " Average training loss: 0.65\n",
            " Validation Accuracy: 0.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 51/51 [00:18<00:00,  2.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.65\n",
            " Average training loss: 0.65\n",
            " Validation Accuracy: 0.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 51/51 [00:18<00:00,  2.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.66\n",
            " Average training loss: 0.63\n",
            " Validation Accuracy: 0.63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 51/51 [00:18<00:00,  2.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.70\n",
            " Average training loss: 0.58\n",
            " Validation Accuracy: 0.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 51/51 [00:18<00:00,  2.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.83\n",
            " Average training loss: 0.42\n",
            " Validation Accuracy: 0.58\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 51/51 [00:18<00:00,  2.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.91\n",
            " Average training loss: 0.27\n",
            " Validation Accuracy: 0.57\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 51/51 [00:18<00:00,  2.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.93\n",
            " Average training loss: 0.21\n",
            " Validation Accuracy: 0.57\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 51/51 [00:18<00:00,  2.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.95\n",
            " Average training loss: 0.16\n",
            " Validation Accuracy: 0.61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 51/51 [00:18<00:00,  2.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.95\n",
            " Average training loss: 0.13\n",
            " Validation Accuracy: 0.61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 51/51 [00:18<00:00,  2.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average training accuracy: 0.95\n",
            " Average training loss: 0.11\n",
            " Validation Accuracy: 0.59\n"
          ]
        }
      ],
      "source": [
        "train_loss_set = []\n",
        "train_accuracy_set = []\n",
        "validation_accuracy_set = []\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
        "# model.dropout = nn.Dropout(0.2, inplace=False)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(768, 384),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(384,2),\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    total_train_accuracy = 0\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Epoch {}\".format(epoch))):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        logits = outputs.logits\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to(device).cpu().numpy()\n",
        "\n",
        "        total_train_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    train_accuracy_set.append(avg_train_accuracy)\n",
        "    train_loss_set.append(avg_train_loss)\n",
        "\n",
        "    print(\" Average training accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "    print(\" Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    validation_accuracy_set.append(avg_val_accuracy)\n",
        "    print(\" Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQSqFjTl0PZ3"
      },
      "source": [
        "## Начинаем с простых методов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "rpa1XSbhvWCK"
      },
      "outputs": [],
      "source": [
        "res['target'] = res['target'].apply(lambda row: target_labeler(row))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b9d9oalkrcy"
      },
      "outputs": [],
      "source": [
        "vocab_size = 15000\n",
        "input_len = 300\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 2\n",
        "n_epochs = 5\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv3At8Li3w1U"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWtNNVEcXi16"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes, num_layers):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        embedded_X = self.embedding(X)\n",
        "        hidden_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(X.device)\n",
        "        cell_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(X.device)\n",
        "        out, _ = self.lstm(embedded_X, (hidden_states, cell_states))\n",
        "        out = self.output_layer(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "f-YB_bYUZPgr"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(res.drop(pos_filtering_bad_idx)['comment'], res.drop(pos_filtering_bad_idx)['target'], random_state=42, test_size=.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Ifz54bFaSxbO"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Создание словаря слов\n",
        "word_dict = defaultdict(lambda: len(word_dict))\n",
        "PAD_IDX = word_dict['<PAD>']  # Паддинг для выравнивания длины последовательностей\n",
        "\n",
        "# Создаем индекс слова для каждого уникального слова в корпусе\n",
        "for text in X_train:  # предполагается, что `your_texts` это список списков слов\n",
        "    for word in text:\n",
        "        _ = word_dict[word]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "hnkgjlqZTIk9"
      },
      "outputs": [],
      "source": [
        "# Функция для конвертации токенов в индексы с установленной длиной\n",
        "def encode_text(text, max_length):\n",
        "    encoded = [word_dict[word] for word in text]\n",
        "    if len(encoded) < max_length:\n",
        "        # Добавляем PAD_IDX для тех случаев, когда длина текста меньше max_length\n",
        "        encoded += [PAD_IDX] * (max_length - len(encoded))\n",
        "    return encoded[:max_length]  # Обрезаем текст до нужной длины, если он слишком длинный\n",
        "\n",
        "max_length = 300  # Установленная длина последовательности\n",
        "encoded_texts_train = [encode_text(text, max_length) for text in X_train]\n",
        "encoded_texts_test = [encode_text(text, max_length) for text in X_test]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDOWTXQEFMPF",
        "outputId": "bb2cbe6b-57bf-4076-9590-33fc8fe38718"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 2,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 1,\n",
              " 5,\n",
              " 8,\n",
              " 3,\n",
              " 9,\n",
              " 6,\n",
              " 10,\n",
              " 11,\n",
              " 9,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 6,\n",
              " 15,\n",
              " 6,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 6,\n",
              " 18,\n",
              " 20,\n",
              " 6,\n",
              " 21,\n",
              " 14,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 6,\n",
              " 25,\n",
              " 16,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 19,\n",
              " 23,\n",
              " 24,\n",
              " 6,\n",
              " 30,\n",
              " 29,\n",
              " 26,\n",
              " 31,\n",
              " 32,\n",
              " 16,\n",
              " 13,\n",
              " 6,\n",
              " 13,\n",
              " 6,\n",
              " 9,\n",
              " 20,\n",
              " 33,\n",
              " 29,\n",
              " 34,\n",
              " 14,\n",
              " 12,\n",
              " 17,\n",
              " 35,\n",
              " 14,\n",
              " 19,\n",
              " 33,\n",
              " 36,\n",
              " 6,\n",
              " 37,\n",
              " 6,\n",
              " 38,\n",
              " 32,\n",
              " 16,\n",
              " 6,\n",
              " 19,\n",
              " 33,\n",
              " 6,\n",
              " 26,\n",
              " 17,\n",
              " 18,\n",
              " 13,\n",
              " 18,\n",
              " 32,\n",
              " 33,\n",
              " 27,\n",
              " 39,\n",
              " 19,\n",
              " 16,\n",
              " 40,\n",
              " 6,\n",
              " 13,\n",
              " 33,\n",
              " 17,\n",
              " 39,\n",
              " 6,\n",
              " 3,\n",
              " 14,\n",
              " 13,\n",
              " 31,\n",
              " 14,\n",
              " 20,\n",
              " 6,\n",
              " 15,\n",
              " 6,\n",
              " 38,\n",
              " 32,\n",
              " 16,\n",
              " 6,\n",
              " 18,\n",
              " 21,\n",
              " 32,\n",
              " 16,\n",
              " 29,\n",
              " 18,\n",
              " 41,\n",
              " 33,\n",
              " 21,\n",
              " 31,\n",
              " 14,\n",
              " 28,\n",
              " 6,\n",
              " 29,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 14,\n",
              " 6,\n",
              " 38,\n",
              " 32,\n",
              " 16,\n",
              " 42,\n",
              " 16,\n",
              " 6,\n",
              " 20,\n",
              " 14,\n",
              " 22,\n",
              " 33,\n",
              " 41,\n",
              " 14,\n",
              " 32,\n",
              " 33,\n",
              " 27,\n",
              " 39,\n",
              " 19,\n",
              " 16,\n",
              " 42,\n",
              " 16,\n",
              " 6,\n",
              " 30,\n",
              " 29,\n",
              " 26,\n",
              " 31,\n",
              " 32,\n",
              " 14,\n",
              " 36,\n",
              " 6,\n",
              " 9,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 6,\n",
              " 25,\n",
              " 16,\n",
              " 27,\n",
              " 33,\n",
              " 20,\n",
              " 19,\n",
              " 14,\n",
              " 40,\n",
              " 6,\n",
              " 31,\n",
              " 14,\n",
              " 31,\n",
              " 6,\n",
              " 17,\n",
              " 27,\n",
              " 28,\n",
              " 6,\n",
              " 22,\n",
              " 26,\n",
              " 35,\n",
              " 21,\n",
              " 31,\n",
              " 16,\n",
              " 42,\n",
              " 16,\n",
              " 40,\n",
              " 6,\n",
              " 32,\n",
              " 14,\n",
              " 31,\n",
              " 6,\n",
              " 18,\n",
              " 6,\n",
              " 17,\n",
              " 27,\n",
              " 28,\n",
              " 6,\n",
              " 35,\n",
              " 33,\n",
              " 19,\n",
              " 21,\n",
              " 31,\n",
              " 16,\n",
              " 42,\n",
              " 16,\n",
              " 6,\n",
              " 20,\n",
              " 17,\n",
              " 16,\n",
              " 29,\n",
              " 16,\n",
              " 13,\n",
              " 39,\n",
              " 28,\n",
              " 36,\n",
              " 6,\n",
              " 43,\n",
              " 44,\n",
              " 16,\n",
              " 27,\n",
              " 16,\n",
              " 32,\n",
              " 16,\n",
              " 33,\n",
              " 6,\n",
              " 28,\n",
              " 34,\n",
              " 27,\n",
              " 16,\n",
              " 31,\n",
              " 16,\n",
              " 45,\n",
              " 6,\n",
              " 25,\n",
              " 16,\n",
              " 22,\n",
              " 16,\n",
              " 42,\n",
              " 14,\n",
              " 33,\n",
              " 32,\n",
              " 6,\n",
              " 21,\n",
              " 16,\n",
              " 24,\n",
              " 29,\n",
              " 14,\n",
              " 19,\n",
              " 18,\n",
              " 32,\n",
              " 39,\n",
              " 6,\n",
              " 22,\n",
              " 16,\n",
              " 27,\n",
              " 16,\n",
              " 17,\n",
              " 16,\n",
              " 21,\n",
              " 32,\n",
              " 39,\n",
              " 6,\n",
              " 18,\n",
              " 6,\n",
              " 31,\n",
              " 29,\n",
              " 14,\n",
              " 21,\n",
              " 16,\n",
              " 32,\n",
              " 26,\n",
              " 40,\n",
              " 6,\n",
              " 21,\n",
              " 32,\n",
              " 29,\n",
              " 16,\n",
              " 12,\n",
              " 19,\n",
              " 16,\n",
              " 21,\n",
              " 32,\n",
              " 39,\n",
              " 6,\n",
              " 30,\n",
              " 18,\n",
              " 42,\n",
              " 26,\n",
              " 29,\n",
              " 23,\n",
              " 6,\n",
              " 18,\n",
              " 6]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_texts_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y97aQeit-qW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC6HxtRgf232",
        "outputId": "75b3af50-a614-49d3-9370-1844f242be22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1256    yes\n",
              "1729     no\n",
              "679     yes\n",
              "551     yes\n",
              "1244    yes\n",
              "       ... \n",
              "1572     no\n",
              "1621    yes\n",
              "1866    yes\n",
              "1226     no\n",
              "1617     no\n",
              "Name: target, Length: 1106, dtype: object"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "PuqqUA9HZyOW"
      },
      "outputs": [],
      "source": [
        "# Преобразование данных в тензоры PyTorch\n",
        "# labels_train = torch.tensor(utils.to_categorical(y_train - 1, 2), dtype=torch.float32)\n",
        "# labels_test = torch.tensor(utils.to_categorical(y_test - 1, 2), dtype=torch.float32)\n",
        "labels_train = torch.tensor(y_train.to_numpy())\n",
        "labels_test = torch.tensor(y_test.to_numpy())\n",
        "\n",
        "inputs_train = torch.tensor(encoded_texts_train, dtype=torch.long)\n",
        "inputs_test = torch.tensor(encoded_texts_test, dtype=torch.long)\n",
        "\n",
        "# Создание тензорного датасета\n",
        "dataset_train = TensorDataset(inputs_train, labels_train)\n",
        "dataset_test = TensorDataset(inputs_test, labels_test)\n",
        "\n",
        "# Создание DataLoader\n",
        "batch_size = 32  # Размер батча\n",
        "train_data_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "test_data_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "kH9FVa8KgOiy",
        "outputId": "a2912e40-3e69-4d8f-e878-a96535d5eee3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_data_loader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-f880a867f797>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data_loader' is not defined"
          ]
        }
      ],
      "source": [
        "next(iter(train_data_loader))[1].type()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t3vd4H_xgv3Y",
        "outputId": "21665fe5-8af0-4bee-c789-2008532b839f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'torch.LongTensor'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_loader))[0].type()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy6uHCfrtqvQ"
      },
      "outputs": [],
      "source": [
        "model = LSTM(vocab_size, input_len, hidden_size, num_classes, num_layers)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWWFxxLD3w1V"
      },
      "outputs": [],
      "source": [
        "model.output_layer = nn.Sequential(\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(128,2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Aw45K-NTa0bz",
        "outputId": "541a8e16-1adb-4bab-a22a-ff7037fb783d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN: Epoch 1, Average Loss: 0.6852, Accuracy: 0.5093\n",
            "TEST: Epoch 1, Average Loss: 0.6784, Accuracy: 0.4947\n",
            "==============================\n",
            "TRAIN: Epoch 2, Average Loss: 0.6600, Accuracy: 0.5694\n",
            "TEST: Epoch 2, Average Loss: 0.6921, Accuracy: 0.5018\n",
            "==============================\n",
            "TRAIN: Epoch 3, Average Loss: 0.6413, Accuracy: 0.6172\n",
            "TEST: Epoch 3, Average Loss: 0.7232, Accuracy: 0.4735\n",
            "==============================\n",
            "TRAIN: Epoch 4, Average Loss: 0.6034, Accuracy: 0.6658\n",
            "TEST: Epoch 4, Average Loss: 0.7675, Accuracy: 0.5053\n",
            "==============================\n",
            "TRAIN: Epoch 5, Average Loss: 0.5456, Accuracy: 0.7241\n",
            "TEST: Epoch 5, Average Loss: 0.8060, Accuracy: 0.4735\n",
            "==============================\n",
            "TRAIN: Epoch 6, Average Loss: 0.4624, Accuracy: 0.7798\n",
            "TEST: Epoch 6, Average Loss: 0.9699, Accuracy: 0.5053\n",
            "==============================\n",
            "TRAIN: Epoch 7, Average Loss: 0.3874, Accuracy: 0.8347\n",
            "TEST: Epoch 7, Average Loss: 1.0928, Accuracy: 0.4912\n",
            "==============================\n",
            "TRAIN: Epoch 8, Average Loss: 0.2730, Accuracy: 0.8904\n",
            "TEST: Epoch 8, Average Loss: 1.4165, Accuracy: 0.5194\n",
            "==============================\n",
            "TRAIN: Epoch 9, Average Loss: 0.1937, Accuracy: 0.9293\n",
            "TEST: Epoch 9, Average Loss: 1.4826, Accuracy: 0.5088\n",
            "==============================\n",
            "TRAIN: Epoch 10, Average Loss: 0.1293, Accuracy: 0.9487\n",
            "TEST: Epoch 10, Average Loss: 1.8753, Accuracy: 0.5088\n",
            "==============================\n",
            "TRAIN: Epoch 11, Average Loss: 0.0992, Accuracy: 0.9629\n",
            "TEST: Epoch 11, Average Loss: 1.8277, Accuracy: 0.4770\n",
            "==============================\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[58], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[27], line 14\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     12\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(X\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     13\u001b[0m cell_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(X\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 14\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(embedded_X, (hidden_states, cell_states))\n\u001b[1;32m     15\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    879\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    model.train()  # Переключение в режим обучения\n",
        "    for inputs, targets in train_data_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted_classes = torch.max(outputs, 1)\n",
        "        correct_predictions = (predicted_classes == targets).float()\n",
        "        total_correct += correct_predictions.sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "    average_loss = total_loss / len(train_data_loader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"TRAIN: Epoch {epoch + 1}, Average Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted_classes = torch.max(outputs, 1)\n",
        "            correct_predictions = (predicted_classes == targets).float()\n",
        "            total_correct += correct_predictions.sum().item()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "    average_loss = total_loss / len(test_data_loader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"TEST: Epoch {epoch + 1}, Average Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    print('='*30)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
